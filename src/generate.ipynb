{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "import jsonlines\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "from nltk.corpus import wordnet as wn\n",
    "import os\n",
    "from py4j.java_gateway import JavaGateway\n",
    "\n",
    "import nodes\n",
    "import relations\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binding namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Namespace, Graph, Literal, URIRef, BNode\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD, DCTERMS\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "ONTOLEX = Namespace(\"http://www.w3.org/ns/lemon/ontolex#\")\n",
    "VARTRANS = Namespace(\"http://www.w3.org/ns/lemon/vartrans#\")\n",
    "LEXINFO = Namespace(\"http://www.lexinfo.net/ontology/2.0/lexinfo#\")\n",
    "LIME = Namespace(\"http://www.w3.org/ns/lemon/lime#\")\n",
    "WORDNET = Namespace(\"https://globalwordnet.github.io/schemas/wn#\")\n",
    "LEXVO = Namespace(\"http://lexvo.org/id/term/\")\n",
    "LVONT = Namespace(\"http://lexvo.org/ontology#\")\n",
    "UWN = Namespace(\"http://www.lexvo.org/uwn/entity/s/\")\n",
    "LILA = Namespace(\"http://lila-erc.eu/ontologies/lila/\")\n",
    "SKOS = Namespace(\"http://www.w3.org/2008/05/skos#\")\n",
    "\n",
    "WIKIENTITY = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "WIKIPROP = Namespace(\"http://www.wikidata.org/prop/direct/\")\n",
    "WIKIBASE = Namespace(\"http://wikiba.se/ontology#\")\n",
    "\n",
    "DUMMY = Namespace(\"http://dummy.com/\")\n",
    "\n",
    "g.bind(\"rdf\", RDF)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dct\", DCTERMS)\n",
    "g.bind(\"owl\", OWL)\n",
    "\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"ontolex\", ONTOLEX)\n",
    "g.bind(\"vartrans\", VARTRANS)\n",
    "g.bind(\"lexinfo\", LEXINFO)\n",
    "g.bind(\"lime\", LIME)\n",
    "g.bind(\"wn\", WORDNET)\n",
    "g.bind(\"lexvo\", LEXVO)\n",
    "g.bind(\"lvont\", LVONT)\n",
    "g.bind(\"uwn\", UWN)\n",
    "g.bind(\"lila\", LILA)\n",
    "g.bind(\"skos\", SKOS)\n",
    "\n",
    "g.bind(\"wd\", WIKIENTITY)\n",
    "g.bind(\"wdt\", WIKIPROP)\n",
    "g.bind(\"wikibase\", WIKIBASE)\n",
    "\n",
    "g.bind(\"dummy\", DUMMY)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "llkg = URIRef(DUMMY.LLKG)\n",
    "llkgGraph = '../data/llkg/llkg.ttl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupGraph():\n",
    "    g.add((llkg, RDF.type, LIME.Lexicon))\n",
    "    g.add((llkg, RDFS.label, Literal('Linked Linguistic Knowledge Graph', lang='en')))\n",
    "    g.add((llkg, SCHEMA.email, Literal('e.ghizzota@studenti.uniba.it')))\n",
    "\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - EtymWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "etymFolder = '../data/etymwn' \n",
    "\n",
    "def languageNodes():\n",
    "    logger.info('Generating language nodes...')\n",
    "\n",
    "    l = Graph()\n",
    "    l.parse(os.path.join(etymFolder, 'lexvo/lexvo_2013-02-09.nt'))\n",
    "    for item in l.subjects(predicate=RDF.type, object=LVONT.Language):\n",
    "        nodes.addLanguageNode(item, l, g)\n",
    "    \n",
    "    g.add((llkg, DCTERMS.language, g.value(subject=None, predicate=RDFS.label, object=Literal(\"English\", lang='en'))))\n",
    "    l.close()\n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def etymNodes():\n",
    "    logger.info('Generating words nodes...')\n",
    "\n",
    "    file = open(os.path.join(etymFolder, 'words.csv'), mode='r', encoding='utf-8')\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        nodes.addEtymLexicalEntryNode(line, g)\n",
    "        g.serialize(format='ttl')\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etymRelations():\n",
    "    logger.info('Connecting nodes...')\n",
    "\n",
    "    file = open(os.path.join(etymFolder,'relations.csv'), 'r')\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    for line in reader:\n",
    "        subj = g.value(predicate=DCTERMS.identifier, object=Literal(line[0], datatype=XSD.string))\n",
    "        obj = g.value(predicate=DCTERMS.identifier, object=Literal(line[2], datatype=XSD.string))\n",
    "\n",
    "        property = line[1]\n",
    "\n",
    "        if property == 'etymology':\n",
    "            relations.addEtymology(subj, obj, g)\n",
    "            relations.addEtymologicalOrigin(obj, subj, g)           \n",
    "        elif property == 'etymologically_related':\n",
    "            relations.addEtymologicallyRelated(subj, obj, g)\n",
    "        elif property == 'has_derived_form':\n",
    "            relations.addHasDerivedForm(subj, obj, g)\n",
    "            relations.addIsDerivedFrom(subj, obj, g)\n",
    "        elif property == 'variant:orthography':\n",
    "            relations.addOrthographyVariant(subj, obj, g)\n",
    "        g.serialize(format='ttl')  \n",
    "    file.close()\n",
    "\n",
    "    logger.info('Nodes successfully connected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 00:07:56,345 Generating language nodes...\n",
      "2024-03-26 00:10:08,987 Serializing...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nd0e565e5e05549748d10d6834730c0e3 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(nodes)\n",
    "importlib.reload(relations)\n",
    "\n",
    "g.remove((None, None, None))\n",
    "\n",
    "setupGraph()\n",
    "languageNodes()\n",
    "#etymNodes()\n",
    "\n",
    "g.serialize(destination=llkgGraph, format='ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - LKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkgDataset = '../data/lkg/dataset.jsonl'\n",
    "wikidataMap = '../data/lkg/wikidata_metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmaNodes():\n",
    "    logger.info('Generating lemma nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:   \n",
    "        lemmas = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Lemma')\n",
    "        for line in lemmas:     \n",
    "            nodes.addFormNode(line, g)      \n",
    "        lkg.close()\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "    \n",
    "def entryNodes():\n",
    "    logger.info('Generating entries nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalEntries = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'InflectedWord')  \n",
    "        for line in lexicalEntries:\n",
    "            nodes.addLexicalEntryNode(line, llkg, g)\n",
    "        lkg.close()\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def resourceNodes():\n",
    "    logger.info('Generating resources nodes...')\n",
    "    nodes.addResourceNode('https://lila-erc.eu/data/lexicalResources/LewisShort/Lexicon', 'Lewis-Short Dictionary', g)\n",
    "    nodes.addResourceNode('https://lila-erc.eu/data/lexicalResources/LatinWordNet/Lexicon', 'Latin WordNet', g)\n",
    "    nodes.addResourceNode('https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/uwn', 'Universal WordNet', g)\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def senseNodes(): \n",
    "    logger.info('Generating lexical sense nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalSenses = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'LexiconConcept')\n",
    "        for line in lexicalSenses:\n",
    "            nodes.addLexicalSenseNode(line, g) \n",
    "        lkg.close()\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def authorNodes():\n",
    "    logger.info('Generating author nodes...')\n",
    "    authors_df = pd.read_csv(os.path.join(wikidataMap, 'latinISE_author_mapping.tsv'), sep='\\t', header=None, usecols=[2,3,4,5], names=['name', 'lastname', 'title', 'id'])\n",
    "    authors_df = authors_df.drop_duplicates(subset=['id'])\n",
    "    authors_df = authors_df.fillna('')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        authors = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Person']     \n",
    "        for line in authors:\n",
    "           nodes.addPersonNode(line, authors_df, g)\n",
    "        lkg.close()\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def occupationNodes():\n",
    "    logger.info('Creating dictionary...')\n",
    "    file = open(os.path.join(wikidataMap, 'occupations_map.tsv'), encoding='utf-8', mode='r')\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    occupationDict = {}\n",
    "    for row in reader:\n",
    "        occupationDict[row[1]] = row[0]\n",
    "    file.close()\n",
    "    logger.info('Dictionary created')\n",
    "\n",
    "    logger.info('Generating occupation nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occupations = [line for line in lkg if line['jtype']=='node' and line['label']=='Occupation']        \n",
    "        for line in occupations:\n",
    "           nodes.addOccupationNode(line, occupationDict, g)\n",
    "        lkg.close()\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def textNodes():\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        ids = [line['object'] for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'HAS_OCCURRENCE']\n",
    "        lkg.close()\n",
    "    logger.info('Generating authors nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occurrences = [line for line in lkg if line['jtype'] == 'node' and line['identity'] in ids]\n",
    "        textID = 1\n",
    "        for line in occurrences:\n",
    "            nodes.addQuotationNode(line, textID, g)\n",
    "            textID = textID + 1\n",
    "        lkg.close()\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def documentNodes():\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        documents = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Document']\n",
    "\n",
    "        for line in documents:\n",
    "            nodes.addCreativeWorkNode(line, g)\n",
    "\n",
    "def corpusNodes():\n",
    "    logger.info('Generating corpora nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        corpora = [line for line in lkg if line['jtype'] == 'node' and line['identity'] == 'Corpus']\n",
    "        for line in corpora:\n",
    "            nodes.addCollectionNode(line, g)      \n",
    "        lkg.close() \n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lkgRelations():\n",
    "\n",
    "    logger.info('Connecting nodes...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        relationships = [line for line in lkg if line['jtype'] == 'relationship']\n",
    "\n",
    "        for line in relationships:\n",
    "            property = line['name']\n",
    "            subj = g.value(predicate=DUMMY.lkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            obj = g.value(predicate=DUMMY.lkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "\n",
    "            if property == 'HAS_LEMMA':    \n",
    "                relations.addCanonicalForm(subj, obj, g)\n",
    "            elif property == 'HAS_CONCEPT':\n",
    "                relations.addSense(subj, obj, g)\n",
    "            elif property == 'HAS_SUBCLASS':\n",
    "                relations.addSenseRel(subj, obj, g)        \n",
    "            elif property == 'SAME_AS':\n",
    "                relations.addSameAs(subj, obj, g)\n",
    "            elif property == 'HAS_OCCURRENCE':\n",
    "                relations.addDCTIsPartOf(subj, obj, g)\n",
    "            elif property == 'HAS_EXAMPLE':\n",
    "                relations.addExample(subj, obj, line, g)\n",
    "            elif property == 'HAS_AUTHOR':\n",
    "                relations.addAuthor(subj, obj, g)\n",
    "            elif property == 'HAS_OCCUPATION':\n",
    "                relations.addHasOccupation(subj, obj, g)\n",
    "            elif property == 'PUBLISHED_IN':\n",
    "                relations.addDatePublished(subj, obj, g)\n",
    "            elif property == 'BELONG_TO':\n",
    "                relations.addSCHEMAIsPartOf(subj, obj, g)\n",
    "        lkg.close\n",
    "\n",
    "    logger.info('Nodes successfully connected!')\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 23:45:06,680 Generating lemma nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 23:45:12,539 Serializing...\n",
      "2024-03-25 23:45:12,551 Generating entries nodes...\n",
      "2024-03-25 23:45:12,656 Serializing...\n",
      "2024-03-25 23:45:12,748 Generating resources nodes...\n",
      "2024-03-25 23:45:12,749 Serializing...\n",
      "2024-03-25 23:45:12,853 Generating lexical sense nodes...\n",
      "2024-03-25 23:45:13,162 Serializing...\n",
      "2024-03-25 23:45:13,381 Connecting nodes...\n",
      "2024-03-25 23:45:14,919 Nodes successfully connected!\n",
      "2024-03-25 23:45:14,920 Serializing...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N6e02b9ef567a40f8bbdaf023d35150c3 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.remove((None, None, None))\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "g.serialize(destination=llkgGraph,format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
