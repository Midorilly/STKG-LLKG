{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "import jsonlines\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binding namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Namespace, Graph, Literal, URIRef, BNode\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD, DCTERMS\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "SCHEMA = Namespace(\"https://schema.org/\")\n",
    "ONTOLEX = Namespace(\"http://www.w3.org/ns/lemon/ontolex#\")\n",
    "VARTRANS = Namespace(\"http://www.w3.org/ns/lemon/vartrans#\")\n",
    "LEXINFO = Namespace(\"http://www.lexinfo.net/ontology/2.0/lexinfo#\")\n",
    "LIME = Namespace(\"http://www.w3.org/ns/lemon/lime#\")\n",
    "WORDNET = Namespace(\"https://globalwordnet.github.io/schemas/wn#\")\n",
    "LEXVO = Namespace(\"http://lexvo.org/id/term/\")\n",
    "LVONT = Namespace(\"http://lexvo.org/ontology#\")\n",
    "LILA = Namespace(\"http://lila-erc.eu/ontologies/lila/\")\n",
    "SKOS = Namespace(\"http://www.w3.org/2008/05/skos#\")\n",
    "\n",
    "WIKIENTITY = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "WIKIPROP = Namespace(\"http://www.wikidata.org/prop/direct/\")\n",
    "WIKIBASE = Namespace(\"http://wikiba.se/ontology#\")\n",
    "\n",
    "DUMMY = Namespace(\"http://dummy.com/\")\n",
    "\n",
    "g.bind(\"rdf\", RDF)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dct\", DCTERMS)\n",
    "g.bind(\"owl\", OWL)\n",
    "\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"ontolex\", ONTOLEX)\n",
    "g.bind(\"vartrans\", VARTRANS)\n",
    "g.bind(\"lexinfo\", LEXINFO)\n",
    "g.bind(\"lime\", LIME)\n",
    "g.bind(\"wn\", WORDNET)\n",
    "g.bind(\"lexvo\", LEXVO)\n",
    "g.bind(\"lvont\", LVONT)\n",
    "g.bind(\"lila\", LILA)\n",
    "g.bind(\"skos\", SKOS)\n",
    "\n",
    "g.bind(\"wd\", WIKIENTITY)\n",
    "g.bind(\"wdt\", WIKIPROP)\n",
    "g.bind(\"wikibase\", WIKIBASE)\n",
    "\n",
    "g.bind(\"dummy\", DUMMY)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "llkg = URIRef(DUMMY.LLKG)\n",
    "llkgGraph = '../data/llkg/llkg.ttl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupGraph():\n",
    "    g.add((llkg, RDF.type, LIME.Lexicon))\n",
    "    g.add((llkg, RDFS.label, Literal('Linked Linguistic Knowledge Graph', lang='en')))\n",
    "    g.add((llkg, SCHEMA.email, Literal('e.ghizzota@studenti.uniba.it')))\n",
    "\n",
    "def updateEntry(entry):\n",
    "    g.add((llkg, LIME.entry, entry))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define SPARQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_query = (\"\"\" SELECT ?lemma \n",
    "    WHERE {\n",
    "        SERVICE <https://lila-erc.eu/sparql/lila_knowledge_base/sparql> {\n",
    "            ?lemma ontolex:writtenRep ?entry ;\n",
    "            lila:hasPOS ?pos .             \n",
    "        }\n",
    "    }\"\"\")\n",
    "\n",
    "document_query = '''SELECT ?document\n",
    "    WHERE {{\n",
    "        VALUES ?title {{ \"{}\"@la}}\n",
    "        ?document wdt:P31 wd:Q7725634 ;\n",
    "           wdt:P1476 ?title.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}   \n",
    "    }} ''' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - EtymWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "etymFolder = '../data/etymwn' \n",
    "\n",
    "def languageNodes():\n",
    "    logger.info('Generating language nodes...')\n",
    "\n",
    "    l = Graph()\n",
    "    l.parse(os.path.join(etymFolder, 'lexvo/lexvo_2013-02-09.nt'))\n",
    "    \n",
    "    for subj in l.subjects(predicate=RDF.type, object=LVONT.Language):\n",
    "        language = URIRef(str(subj))\n",
    "        g.add((language, RDF.type, DCTERMS.LinguisticSystem))\n",
    "        g.add((language, RDFS.label, Literal(l.value(subject=subj, predicate=SKOS.prefLabel, object=None), lang='en')))\n",
    "        g.add((language, DUMMY.iso6391, Literal(l.value(subject=subj, predicate=LVONT.iso639P1Code, object=None, any=False), datatype=XSD.string)))\n",
    "        g.add((language, DUMMY.iso6392, Literal(l.value(subject=subj, predicate=LVONT.iso6392TCode, object=None, any=False), datatype=XSD.string)))\n",
    "        g.add((language, DUMMY.iso6393, Literal(l.value(subject=subj, predicate=LVONT.iso639P3PCode, object=None, any=False), datatype=XSD.string)))\n",
    "\n",
    "    g.add((llkg, DCTERMS.language, g.value(subject=None, predicate=RDFS.label, object=Literal(\"English\", lang='en'))))\n",
    "    l.close()\n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def etymNodes():\n",
    "    logger.info('Generating words nodes...')\n",
    "\n",
    "    file = open(os.path.join(etymFolder, 'words.csv'), mode='r', encoding='utf-8')\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    for value in reader:\n",
    "        wordString = str(value[1])\n",
    "        word = URIRef(LEXVO+value[2]+'/'+quote(wordString))\n",
    "        if not (word, None, None) in g:\n",
    "            updateEntry(word)     \n",
    "            if bool(re.search(r'\\s', wordString)):\n",
    "                g.add((word, RDF.type, ONTOLEX.MultiwordExpression))\n",
    "            elif wordString.startswith('-') or wordString.endswith('-'):\n",
    "                g.add((word, RDF.type, ONTOLEX.Affix))\n",
    "            else:\n",
    "                g.add((word, RDF.type, ONTOLEX.Word))\n",
    "            g.add((word, RDFS.label, Literal(wordString, datatype=XSD.string)))\n",
    "            g.add((word, DCTERMS.identifier, Literal(value[0], datatype=XSD.string)))\n",
    "            g.add((word, DCTERMS.language, Literal(g.value(subject=None, predicate=DUMMY.iso6393, object=Literal(value[2], datatype=XSD.string)))))\n",
    "        else:\n",
    "            g.add((word, DCTERMS.identifier, Literal(value[0], datatype=XSD.string)))\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etymRelations():\n",
    "    logger.info('Connecting nodes...')\n",
    "\n",
    "    file = open(os.path.join(etymFolder,'relations.csv'), 'r')\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    for line in reader:\n",
    "        subj = g.value(predicate=DCTERMS.identifier, object=Literal(line[0], datatype=XSD.string))\n",
    "        obj = g.value(predicate=DCTERMS.identifier, object=Literal(line[2], datatype=XSD.string))\n",
    "\n",
    "        relation = line[1]\n",
    "\n",
    "        if relation == 'etymology':\n",
    "            g.add((URIRef(str(subj)), DUMMY.etymology, URIRef(str(obj))))\n",
    "        elif relation == 'etymological_origin_of':\n",
    "            g.add((URIRef(str(subj)), DUMMY.etymologicalOriginOf, URIRef(str(obj))))\n",
    "        elif relation == 'etymologically_related':\n",
    "            g.add((URIRef(str(subj)), DUMMY.etymologically_related, URIRef(str(obj))))\n",
    "        elif relation == 'has_derived_form':\n",
    "            g.add((URIRef(str(subj)), DUMMY.has_derived_form, URIRef(str(obj))))\n",
    "            g.add((URIRef(str(obj)), DUMMY.is_derived_from, URIRef(str(subj))))\n",
    "        elif relation == 'variant:orthography':\n",
    "            g.add((URIRef(str(subj)), DUMMY.orthographyVariant, URIRef(str(obj))))\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "    logger.info('Nodes successfully connected!')\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 00:41:33,338 Generating language nodes...\n",
      "2024-03-25 00:42:09,293 Serializing...\n",
      "2024-03-25 00:42:11,230 Generating words nodes...\n"
     ]
    }
   ],
   "source": [
    "#g.remove((None, None, None))\n",
    "\n",
    "languageNodes()\n",
    "etymNodes()\n",
    "\n",
    "g.serialize(destination=llkgGraph, format='ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - LKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkgDataset = '../data/lkg/dataset.jsonl'\n",
    "wikidataMap = '../data/lkg/wikidata_metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin = URIRef(str(g.value(subject=None, predicate=RDFS.label, object=Literal('Latin', lang='en'), any=False)))\n",
    "\n",
    "def lemmaNodes():\n",
    "\n",
    "    logger.info('Generating lemma nodes...')\n",
    "\n",
    "    lilaPosMapping = {'N' : LILA.noun, 'ADJ' : LILA.adjective, 'V' : LILA.verb}\n",
    "    lexinfoPosMapping = {'N' : LEXINFO.noun , 'ADJ' : LEXINFO.adjective, 'V' : LEXINFO.verb}\n",
    "    \n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:   \n",
    "        lemmas = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Lemma')\n",
    "\n",
    "        lemmaID = 1\n",
    "        for line in lemmas:     \n",
    "            result = g.query(lemma_query, initNs = {'ontolex' : ONTOLEX, 'lila': LILA}, initBindings={'entry': Literal(line['properties']['value']), 'pos' : URIRef(lilaPosMapping[line['properties']['posTag']]) })\n",
    "            for r in result:\n",
    "                lemma = r.lemma    \n",
    "                g.add((lemma, RDF.type, ONTOLEX.Form))\n",
    "                g.add((lemma, RDFS.label, Literal(line['properties']['value'])))\n",
    "                g.add((lemma, DCTERMS.identifier, Literal(line['identity'], datatype=XSD.unsignedInt)))\n",
    "                g.add((lemma, DCTERMS.identifier, Literal('lemma_{}'.format(lemmaID), datatype=XSD.string)))\n",
    "                g.add((lemma, ONTOLEX.writtenRep, Literal(line['properties']['value'], lang='la'))) \n",
    "                g.add((lemma, LEXINFO.partOfSpeech, URIRef(lexinfoPosMapping[line['properties']['posTag']])))\n",
    "                g.add((lemma, DCTERMS.language, g.value(subject=None, predicate=RDFS.label, object=Literal(\"Latin\", lang='en'))))\n",
    "                lemmaID = lemmaID + 1\n",
    "        \n",
    "        lkg.close()\n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "    \n",
    "def entryNodes():\n",
    "\n",
    "    logger.info('Generating entries nodes...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalEntries = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'InflectedWord')  \n",
    "        leID = 1\n",
    "        for line in lexicalEntries:\n",
    "            value = line['properties']['value'].lower()\n",
    "            word = URIRef(LEXVO+'lat/'+value)\n",
    "            if not (word, None, None) in g:\n",
    "                updateEntry(word)\n",
    "                if bool(re.search(r'\\s', value)):\n",
    "                    g.add((word, RDF.type, ONTOLEX.MultiwordExpression))\n",
    "                elif value.startswith('-') or value.endswith('-'):\n",
    "                    g.add((word, RDF.type, ONTOLEX.Affix))\n",
    "                else:\n",
    "                    g.add((word, RDF.type, ONTOLEX.Word))\n",
    "                g.add((word, RDFS.label, Literal(value)))\n",
    "                g.add((word, DCTERMS.language, latin))\n",
    "                g.add((word, DCTERMS.identifier, Literal(line['identity'], datatype=XSD.unsignedInt)))\n",
    "                g.add((word, DCTERMS.identifier, Literal('le_{}'.format(leID), datatype=XSD.string)))\n",
    "                leID = leID + 1\n",
    "            else:\n",
    "                g.add((word, DCTERMS.identifier, Literal(line['identity'], datatype=XSD.unsignedInt)))\n",
    "        lkg.close()\n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def senseNodes(): \n",
    "\n",
    "    logger.info('Generating lexical sense nodes...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalSenses = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'LexiconConcept')\n",
    "\n",
    "        LS = URIRef(\"https://lila-erc.eu/data/lexicalResources/LewisShort/Lexicon\")\n",
    "        g.add((LS, RDF.type, RDFS.Resource))\n",
    "        g.add((LS, RDFS.label, Literal('Lewis-Short Dictionary', lang='en')))\n",
    "        LWN = URIRef(\"https://lila-erc.eu/data/lexicalResources/LatinWordNet/Lexicon\")\n",
    "        g.add((LWN, RDF.type, RDFS.Resource))\n",
    "        g.add((LWN, RDFS.label, Literal('Latin WordNet', lang='en')))\n",
    "\n",
    "        lsID = 1\n",
    "        lwnID = 1\n",
    "        for line in lexicalSenses:\n",
    "            if line['properties']['resource'] == 'Lewis-Short Dictionary':\n",
    "                sense = URIRef(line['properties']['id'])\n",
    "                g.add((sense, RDF.type, ONTOLEX.LexicalSense))\n",
    "                g.add((sense, DCTERMS.source, LS))\n",
    "                g.add((sense, DCTERMS.description, Literal(line['properties']['alias'], lang='en')))     \n",
    "                g.add((sense, DCTERMS.identifier, Literal(line['identity'], datatype=XSD.unsignedInt)))\n",
    "                g.add((sense, DCTERMS.identifier, Literal('ls_{}'.format(lsID), datatype=XSD.string)))\n",
    "                lsID = lsID + 1\n",
    "            elif line['properties']['resource'] == 'Latin WordNet':\n",
    "                sense = URIRef(line['properties']['alias'])\n",
    "                g.add((sense, RDF.type, ONTOLEX.LexicalSense))\n",
    "                g.add((sense, DCTERMS.source, LWN))\n",
    "                g.add((sense, DCTERMS.description, Literal(line['properties']['gloss'], lang='en')))\n",
    "                g.add((sense, DCTERMS.identifier, Literal(line['identity'], datatype=XSD.unsignedInt)))\n",
    "                g.add((sense, DCTERMS.identifier, Literal('lwn_{}'.format(lwnID), datatype=XSD.string)))\n",
    "                lwnID = lwnID + 1\n",
    "                \n",
    "        lkg.close()\n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def authorNodes():\n",
    "\n",
    "    logger.info('Generating author nodes...')\n",
    "\n",
    "    authors_df = pd.read_csv(os.path.join(wikidataMap, 'latinISE_author_mapping.tsv'), sep='\\t', header=None, usecols=[2,3,4,5], names=['name', 'lastname', 'title', 'id'])\n",
    "    authors_df = authors_df.drop_duplicates(subset=['id'])\n",
    "    authors_df = authors_df.fillna('')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        authors = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Person']\n",
    "        \n",
    "        for line in authors:\n",
    "            name = line['properties']['name']\n",
    "            lastname = line['properties']['lastname']\n",
    "            if not lastname:\n",
    "                wikiEntity = authors_df.loc[(authors_df['name'] == name), 'id'].values             \n",
    "            else:\n",
    "                wikiEntity = authors_df.loc[((authors_df['name'] == name) & (authors_df['lastname'] == lastname)), 'id'].values\n",
    "            \n",
    "            if wikiEntity.size > 0:\n",
    "                author = URIRef(WIKIENTITY+wikiEntity[0])\n",
    "                g.add((author, RDF.type, SCHEMA.Person))\n",
    "                g.add((Literal(name), RDF.type, SCHEMA.Text))\n",
    "                g.add((author, SCHEMA.givenName, Literal(name)))\n",
    "                if len(lastname)>0:\n",
    "                    g.add((Literal(lastname), RDF.type, SCHEMA.Text))\n",
    "                    g.add((author, SCHEMA.familyName, Literal(lastname)))\n",
    "                g.add((author, DCTERMS.identifier, Literal(line['identity'], datatype=XSD.unsignedInt)))\n",
    "\n",
    "        lkg.close()\n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def occupationNodes():\n",
    "    logger.info('Creating dictionary...')\n",
    "\n",
    "    file = open(os.path.join(wikidataMap, 'occupations_map.tsv'), encoding='utf-8', mode='r')\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    occupationDict = {}\n",
    "    for row in reader:\n",
    "        occupationDict[row[1]] = row[0]\n",
    "    file.close()\n",
    "\n",
    "    logger.info('Dictionary created')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occupations = [line for line in lkg if line['jtype']=='node' and line['label']=='Occupation']\n",
    "\n",
    "        logger.info('Generating occupation nodes...')\n",
    "        \n",
    "        for line in occupations:\n",
    "            value = line['properties']['value']\n",
    "            occupation = URIRef(WIKIENTITY+occupationDict[value])\n",
    "            g.add((occupation, RDF.type, SCHEMA.Occupation))\n",
    "            g.add((occupation, RDFS.label, Literal(value, datatype=XSD.string)))\n",
    "            g.add((occupation, DCTERMS.identifier, Literal(line['identity'], datatype=XSD.unsignedInt)))\n",
    "        lkg.close()\n",
    "\n",
    "    logger.info('Nodes generated')\n",
    "    \n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def textNodes():\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        ids = [line['object'] for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'HAS_OCCURRENCE']\n",
    "        lkg.close()\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occurrences = [line for line in lkg if line['jtype'] == 'node' and line['identity'] in ids]\n",
    "        textID = 1\n",
    "        for line in occurrences:\n",
    "            text = Literal('text_{}'.format(textID))\n",
    "            g.add((text, RDF.type, SCHEMA.Quotation))\n",
    "            g.add((text, SCHEMA.text, Literal(line['properties']['value'], datatype=XSD.string)))\n",
    "            g.add((text, DCTERMS.language, latin)) \n",
    "\n",
    "'''def documentNodes():\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "    documents = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Document']\n",
    "\n",
    "    for line in documents:\n",
    "        #QUERY SPARQL'''\n",
    "\n",
    "def corpusNodes():\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        corpora = [line for line in lkg if line['jtype'] == 'node' and line['identity'] == 'Corpus']\n",
    "        cID = 1\n",
    "        for line in corpora:\n",
    "            corpus = Literal(line['properties']['name'], datatype=XSD.string)\n",
    "            g.add((corpus, RDF.type, SCHEMA.Collection))\n",
    "            g.add((corpus, RDFS.label, Literal('c_{}'.format(cID), datatype=XSD.string)))\n",
    "            cID = cID + 1\n",
    "        lkg.close() \n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lkgRelations():\n",
    "\n",
    "    logger.info('Connecting nodes...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        relations = [line for line in lkg if line['jtype'] == 'relationship']\n",
    "\n",
    "        for line in relations:\n",
    "            relation = line['name']\n",
    "            subj = g.value(predicate=DCTERMS.identifier, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            obj = g.value(predicate=DCTERMS.identifier, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "\n",
    "            if relation == 'HAS_LEMMA':    \n",
    "                if subj != obj :\n",
    "                    g.add((URIRef(str(subj)), ONTOLEX.canonicalForm, URIRef(str(obj))))\n",
    "\n",
    "            elif relation == 'HAS_CONCEPT':\n",
    "                for o in g.objects(subject = subj, predicate=RDF.type):\n",
    "                    if o != ONTOLEX.Form: # according to Ontolex schema, Form entities are not directly linked to LexicalSense entities\n",
    "                        g.add((URIRef(str(subj)), ONTOLEX.sense, URIRef(str(obj))))\n",
    "                        g.add((URIRef(str(obj)), ONTOLEX.isSenseOf, URIRef(str(subj))))\n",
    "\n",
    "            elif relation == 'HAS_SUBCLASS':\n",
    "                g.add((URIRef(str(subj)), VARTRANS.senseRel, URIRef(str(obj))))\n",
    "                \n",
    "            elif relation == 'SAME_AS':\n",
    "                g.add((URIRef(str(subj)), OWL.sameAs, URIRef(str(obj))))\n",
    "\n",
    "            elif relation == 'HAS_OCCUPATION':\n",
    "                g.add((URIRef(str(subj)), SCHEMA.hasOccupation, URIRef(str(obj))))\n",
    "\n",
    "            elif relation == 'HAS_OCCURRENCE':\n",
    "                g.add((URIRef(str(subj)), DCTERMS.isPartOf, URIRef(str(obj))))\n",
    "                #g.add((URIRef(str(obj), POWLA.start, Literal())))              UNAVAILABLE DATA\n",
    "                #g.add((URIRef(str(obj), POWLA.end, Literal())))                UNAVAILABLE DATA\n",
    "\n",
    "            elif relation == 'HAS_EXAMPLE':\n",
    "                g.add((URIRef(str(subj)), WORDNET.example, URIRef(str(obj))))\n",
    "                g.add((URIRef(str(obj), DUMMY.grade, Literal(line['properties']['grade'], datatype=XSD.float))))\n",
    "\n",
    "            elif relation == 'HAS_AUTHOR':\n",
    "                g.add((URIRef(str(subj)), SCHEMA.author, URIRef(str(obj))))\n",
    "\n",
    "            elif relation == 'PUBLISHED_IN':\n",
    "                g.add((URIRef(str(subj)), SCHEMA.datePublished, URIRef(str(obj))))\n",
    "\n",
    "            elif relation == 'BELONG_TO':\n",
    "                g.add((URIRef(str(subj)), SCHEMA.isPartOf, URIRef(str(obj))))\n",
    "\n",
    "        lkg.close\n",
    "\n",
    "    logger.info('Nodes successfully connected!')\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g.remove((None, None, None))\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "'''lemmaNodes()\n",
    "entryNodes()\n",
    "senseNodes()\n",
    "authorNodes()\n",
    "textNodes()\n",
    "corpusNodes()\n",
    "relations()'''\n",
    "\n",
    "g.serialize(destination=llkgGraph,format='ttl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
