{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "import jsonlines\n",
    "#import re\n",
    "#from urllib.parse import quote\n",
    "#from nltk.corpus import wordnet as wn\n",
    "import os\n",
    "from py4j.java_gateway import JavaGateway\n",
    "from rdflib import Graph, Literal, URIRef, BNode\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD, DCTERMS\n",
    "\n",
    "import nodes\n",
    "import relations\n",
    "import queries\n",
    "from namespaces import *\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(nodes)\n",
    "importlib.reload(relations)\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binding namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "\n",
    "g.bind(\"rdf\", RDF)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dct\", DCTERMS)\n",
    "g.bind(\"owl\", OWL)\n",
    "\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"ontolex\", ONTOLEX)\n",
    "g.bind(\"vartrans\", VARTRANS)\n",
    "g.bind(\"lexinfo\", LEXINFO)\n",
    "g.bind(\"lime\", LIME)\n",
    "g.bind(\"wn\", WORDNET)\n",
    "g.bind(\"lexvo\", LEXVO)\n",
    "g.bind(\"lvont\", LVONT)\n",
    "g.bind(\"uwn\", UWN)\n",
    "g.bind(\"lila\", LILA)\n",
    "g.bind(\"skos\", SKOS)\n",
    "\n",
    "g.bind(\"wd\", WIKIENTITY)\n",
    "g.bind(\"wdt\", WIKIPROP)\n",
    "g.bind(\"wikibase\", WIKIBASE)\n",
    "g.bind(\"bd\", BIGDATA)\n",
    "\n",
    "g.bind(\"dummy\", DUMMY)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "llkg = URIRef(DUMMY.LLKG)\n",
    "llkgGraph = '../data/llkg/llkg.ttl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupGraph():\n",
    "    g.add((llkg, RDF.type, LIME.Lexicon))\n",
    "    g.add((llkg, RDFS.label, Literal('Linked Linguistic Knowledge Graph', lang='en')))\n",
    "    g.add((llkg, SCHEMA.email, Literal('e.ghizzota@studenti.uniba.it')))\n",
    "\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - EtymWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "etymFolder = '../data/etymwn' \n",
    "\n",
    "def languageNodes():\n",
    "    logger.info('Generating language nodes...')\n",
    "\n",
    "    l = Graph()\n",
    "    l.parse(os.path.join(etymFolder, 'lexvo/lexvo_2013-02-09.nt'))\n",
    "    for item in l.subjects(predicate=RDF.type, object=LVONT.Language):\n",
    "        nodes.addLanguageNode(language=item, l=l, g=g)\n",
    "    \n",
    "    g.add((llkg, DCTERMS.language, g.value(subject=None, predicate=RDFS.label, object=Literal(\"English\", lang='en'))))\n",
    "    l.close()\n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def etymNodes():\n",
    "    logger.info('Generating words nodes...')\n",
    "\n",
    "    file = open(os.path.join(etymFolder, 'words.csv'), mode='r', encoding='utf-8')\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        nodes.addEtymLexicalEntryNode(word=line[1], language=line[2], iso='3', id=line[0], llkg=llkg, g=g)\n",
    "        g.serialize(format='ttl')\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etymRelations():\n",
    "    logger.info('Connecting nodes...')\n",
    "\n",
    "    file = open(os.path.join(etymFolder,'relations.csv'), 'r')\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    for line in reader:\n",
    "        subj = g.value(predicate=DUMMY.etymwnID, object=Literal(line[0], datatype=XSD.string))\n",
    "        obj = g.value(predicate=DUMMY.etymwnID, object=Literal(line[2], datatype=XSD.string))\n",
    "\n",
    "        property = line[1]\n",
    "\n",
    "        if property == 'etymology':\n",
    "            relations.addEtymology(subj, obj, g)\n",
    "            relations.addEtymologicalOrigin(obj, subj, g)           \n",
    "        elif property == 'etymologically_related':\n",
    "            relations.addEtymologicallyRelated(subj, obj, g)\n",
    "        elif property == 'has_derived_form':\n",
    "            relations.addHasDerivedForm(subj, obj, g)\n",
    "            relations.addIsDerivedFrom(subj, obj, g)\n",
    "        elif property == 'variant:orthography':\n",
    "            relations.addOrthographyVariant(subj, obj, g)\n",
    "        g.serialize(format='ttl')  \n",
    "    file.close()\n",
    "\n",
    "    logger.info('Nodes successfully connected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - LKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkgDataset = '../data/lkg/dataset.jsonl'\n",
    "wikidataMap = '../data/lkg/wikidata_metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resourceNodes():\n",
    "    logger.info('Generating resources nodes...')\n",
    "    nodes.addResourceNode(resource='https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.04.0059', label='Lewis-Short Dictionary', g=g)\n",
    "    nodes.addResourceNode(resource='https://lila-erc.eu/data/lexicalResources/LatinWordNet/Lexicon', label='Latin WordNet', g=g)\n",
    "    nodes.addResourceNode(resource='https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/uwn', label='Universal WordNet', g=g)\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def lemmaNodes():\n",
    "    logger.info('Generating lemma nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:   \n",
    "        lemmas = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Lemma')\n",
    "        for line in lemmas:     \n",
    "            nodes.addFormNode(writtenRep=line['properties']['value'], pos=line['properties']['posTag'], id=line['identity'], g=g)      \n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "    \n",
    "def entryNodes():\n",
    "    logger.info('Generating entries nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalEntries = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'InflectedWord')  \n",
    "        for line in lexicalEntries:\n",
    "            nodes.addLexicalEntryNode(entry=line['properties']['value'], id=line['identity'], language='Latin', llkg=llkg, g=g)\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def senseNodes(): \n",
    "    logger.info('Generating lexical sense nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalSenses = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'LexiconConcept')\n",
    "        for line in lexicalSenses:\n",
    "            resource = line['properties']['resource']\n",
    "            if resource == 'Lewis-Short Dictionary':\n",
    "                nodes.addLexicalSenseNode(resource=resource, sense=line['properties']['id'], gloss=line['properties']['alias'], id=line['identity'], g=g) \n",
    "            elif resource == 'Latin WordNet':\n",
    "                nodes.addLexicalSenseNode(resource='Universal WordNet', sense=line['properties']['alias'], gloss=line['properties']['gloss'], id=line['identity'], g=g) \n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def authorNodes():\n",
    "    logger.info('Generating author nodes...')\n",
    "    authors_df = pd.read_csv(os.path.join(wikidataMap, 'latinISE_author_mapping.tsv'), sep='\\t', header=None, usecols=[2,3,4,5], names=['name', 'lastname', 'title', 'id'])\n",
    "    authors_df = authors_df.drop_duplicates(subset=['id'])\n",
    "    authors_df = authors_df.fillna('')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        authors = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Person']     \n",
    "        for line in authors:\n",
    "           nodes.addPersonNode(firstname=line['properties']['name'], lastname=line['properties']['lastname'], id=line['identity'], df=authors_df, g=g)\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def occupationNodes():\n",
    "    logger.info('Creating dictionary...')\n",
    "    file = open(os.path.join(wikidataMap, 'occupations_map.tsv'), encoding='utf-8', mode='r')\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    occupationDict = {}\n",
    "    for row in reader:\n",
    "        occupationDict[row[1]] = row[0]\n",
    "    file.close()\n",
    "    logger.info('Dictionary created')\n",
    "\n",
    "    logger.info('Generating occupation nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occupations = [line for line in lkg if line['jtype']=='node' and line['label']=='Occupation']        \n",
    "        for line in occupations:\n",
    "           nodes.addOccupationNode(occupation=line['properties']['value'], id=line['identity'], dict=occupationDict, g=g)\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def textNodes():\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        ids = [line['object'] for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'HAS_OCCURRENCE']\n",
    "    logger.info('Generating text nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occurrences = [line for line in lkg if line['jtype'] == 'node' and line['identity'] in ids]\n",
    "        textID = 1\n",
    "        for line in occurrences:\n",
    "            nodes.addQuotationNode(quotation=line['properties']['value'], language='Latin', id=line['identity'], g=g)\n",
    "            textID = textID + 1\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def documentNodes():\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        documents = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Document']\n",
    "        for line in documents:\n",
    "            nodes.addCreativeWorkNode(title=line['properties']['title'], id=line['identity'], g=g)\n",
    "\n",
    "def corpusNodes():\n",
    "    logger.info('Generating corpora nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        corpora = [line for line in lkg if line['jtype'] == 'node' and line['identity'] == 'Corpus']\n",
    "        for line in corpora:\n",
    "            nodes.addCollectionNode(title=line['properties']['name'], id=line['identity'], g=g)      \n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateDictionary():\n",
    "    logger.info('Generating dates dictionary...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        startTimes = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'startTime']\n",
    "        startDict = {}\n",
    "        for line in startTimes:\n",
    "            startDict[line['subject']] = line['object']\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        endTimes = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'endTime']\n",
    "        endDict = {}\n",
    "        for line in endTimes:\n",
    "            endDict[line['subject']] = line['object']\n",
    "        \n",
    "        intervalsDict = {}\n",
    "        for k in startDict.keys():\n",
    "            intervalsDict.update({k : (startDict[k], endDict[k])})\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        timePoints = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'TimePoint']\n",
    "        pointsDict = {}\n",
    "        for line in timePoints:\n",
    "            pointsDict[line['identity']] = line['properties']['Year']\n",
    "\n",
    "    return intervalsDict, pointsDict\n",
    "\n",
    "def lkgRelations():\n",
    "\n",
    "    logger.info('Connecting nodes...')\n",
    "\n",
    "    intervalsDict, pointsDict = dateDictionary()\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        relationships = [line for line in lkg if line['jtype'] == 'relationship']\n",
    "\n",
    "        for line in relationships:\n",
    "            property = line['name']\n",
    "            subj = g.value(predicate=DUMMY.lkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            obj = g.value(predicate=DUMMY.lkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "\n",
    "            if property == 'HAS_LEMMA':    \n",
    "                relations.addCanonicalForm(subj, obj, g)\n",
    "            elif property == 'HAS_CONCEPT':\n",
    "                relations.addSense(subj, obj, g)\n",
    "            elif property == 'HAS_SUBCLASS':\n",
    "                relations.addSenseRel(subj, obj, g)        \n",
    "            elif property == 'SAME_AS':\n",
    "                relations.addSameAs(subj, obj, g)\n",
    "            elif property == 'HAS_AUTHOR':\n",
    "                relations.addAuthor(subj, obj, g)\n",
    "            elif property == 'HAS_OCCUPATION':\n",
    "                relations.addHasOccupation(subj, obj, g)\n",
    "            elif property == 'BELONG_TO':\n",
    "                relations.addSCHEMAIsPartOf(subj, obj, g)\n",
    "            elif property == 'HAS_OCCURRENCE':\n",
    "                occurrence = g.value(subject=None, predicate=DCTERMS.isPartOf, object=obj)\n",
    "                relations.addDCTIsPartOf(subj, occurrence, g)\n",
    "            elif property == 'HAS_EXAMPLE':\n",
    "                example = g.value(subject=None, predicate=DCTERMS.isPartOf, object=obj)\n",
    "                relations.addExample(subj, example, line['properties']['grade'], g)\n",
    "            elif property == 'PUBLISHED_IN' or property == 'BORN' or property == 'DIED':\n",
    "                if line['object'] in intervalsDict.keys():\n",
    "                    s, e = intervalsDict[line['object']]\n",
    "                    start = pointsDict[s]\n",
    "                    end = pointsDict[e]\n",
    "                    relations.addDateInterval(subj, start, end, property, g)\n",
    "                elif line['object'] in pointsDict.keys():\n",
    "                    relations.addDatePoint(subj, pointsDict[line['object']], property, g)\n",
    "\n",
    "    logger.info('Nodes successfully connected!')\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 19:47:24,173 Generating language nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 19:47:58,014 Serializing...\n",
      "2024-03-27 19:47:59,596 Generating resources nodes...\n",
      "2024-03-27 19:47:59,596 Serializing...\n",
      "2024-03-27 19:48:00,729 Generating lemma nodes...\n",
      "2024-03-27 19:48:07,625 Serializing...\n",
      "2024-03-27 19:48:08,741 Generating entries nodes...\n",
      "2024-03-27 19:48:08,841 Serializing...\n",
      "2024-03-27 19:48:10,007 Generating author nodes...\n",
      "2024-03-27 19:48:10,290 Serializing...\n",
      "2024-03-27 19:48:11,571 Creating dictionary...\n",
      "2024-03-27 19:48:11,592 Dictionary created\n",
      "2024-03-27 19:48:11,592 Generating occupation nodes...\n",
      "2024-03-27 19:48:11,689 Serializing...\n",
      "2024-03-27 19:48:12,957 Generating lexical sense nodes...\n",
      "2024-03-27 19:48:13,258 Serializing...\n",
      "2024-03-27 19:48:14,741 Generating text nodes...\n",
      "2024-03-27 19:48:15,233 Serializing...\n",
      "2024-03-27 19:48:17,420 Generating corpora nodes...\n",
      "2024-03-27 19:48:17,498 Serializing...\n",
      "2024-03-27 19:48:19,635 Connecting nodes...\n",
      "2024-03-27 19:48:19,635 Generating dates dictionary...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "addDatePointPublished() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m documentNodes()\n\u001b[0;32m     17\u001b[0m corpusNodes()\n\u001b[1;32m---> 18\u001b[0m \u001b[43mlkgRelations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m g\u001b[38;5;241m.\u001b[39mserialize(destination\u001b[38;5;241m=\u001b[39mllkgGraph,\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mttl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[102], line 69\u001b[0m, in \u001b[0;36mlkgRelations\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m                 relations\u001b[38;5;241m.\u001b[39maddDateInterval(subj, start, end, \u001b[38;5;28mproperty\u001b[39m, g)\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m pointsDict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m---> 69\u001b[0m                 \u001b[43mrelations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddDatePointPublished\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpointsDict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mproperty\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNodes successfully connected!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSerializing...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: addDatePointPublished() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "importlib.reload(nodes)\n",
    "importlib.reload(relations)\n",
    "\n",
    "g.remove((None, None, None))\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "setupGraph()\n",
    "languageNodes()\n",
    "resourceNodes()\n",
    "lemmaNodes()\n",
    "entryNodes()\n",
    "authorNodes()\n",
    "occupationNodes()\n",
    "senseNodes()\n",
    "textNodes()\n",
    "documentNodes()\n",
    "corpusNodes()\n",
    "lkgRelations()\n",
    "\n",
    "\n",
    "g.serialize(destination=llkgGraph,format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINK DOCUMENT TO WIKIDATA VIA TITLE\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def transform2dicts(results):\n",
    "\n",
    "    new_results = []\n",
    "    for result in results:\n",
    "        new_result = {}\n",
    "        for key in result:\n",
    "            new_result[key] = result[key]['value']\n",
    "        new_results.append(new_result)\n",
    "    return new_results\n",
    "\n",
    "\n",
    "endpoint = \"https://query.wikidata.org/sparql\"\n",
    "sparql = SPARQLWrapper(endpoint)\n",
    "sparql.setQuery(queries.documentQuery.format(\"Ab Urbe condĭta\"))\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "results = sparql.queryAndConvert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'head': {'vars': ['document']}, 'results': {'bindings': [{'document': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q1155892'}}]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'head': {'vars': ['document']},\n",
       " 'results': {'bindings': [{'document': {'type': 'uri',\n",
       "     'value': 'http://www.wikidata.org/entity/Q1155892'}}]}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 17:02:37,453 Generating date nodes...\n",
      "2024-03-27 17:02:37,669 Generatin time points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2000 -1901 / -1999 -1900\n",
      "-1900 -1801 / -1899 -1800\n",
      "-1800 -1701 / -1799 -1700\n",
      "-1700 -1601 / -1699 -1600\n",
      "-1600 -1501 / -1599 -1500\n",
      "-1500 -1401 / -1499 -1400\n",
      "-1400 -1301 / -1399 -1300\n",
      "-1300 -1201 / -1299 -1200\n",
      "-1200 -1101 / -1199 -1100\n",
      "-1100 -1001 / -1099 -1000\n",
      "-1000 -901 / -0999 -0900\n",
      "-900 -801 / -0899 -0800\n",
      "-800 -701 / -0799 -0700\n",
      "-700 -601 / -0699 -0600\n",
      "-600 -501 / -0599 -0500\n",
      "-500 -401 / -0499 -0400\n",
      "-400 -301 / -0399 -0300\n",
      "-300 -201 / -0299 -0200\n",
      "-200 -101 / -0199 -0100\n",
      "-100 -1 / -0099 +0000\n",
      "1 100 / +0001 +0100\n",
      "101 200 / +0101 +0200\n",
      "201 300 / +0201 +0300\n",
      "301 400 / +0301 +0400\n",
      "401 500 / +0401 +0500\n",
      "501 600 / +0501 +0600\n",
      "601 700 / +0601 +0700\n",
      "701 800 / +0701 +0800\n",
      "801 900 / +0801 +0900\n",
      "901 1000 / +0901 +1000\n",
      "1001 1100 / +1001 +1100\n",
      "1101 1200 / +1101 +1200\n",
      "1201 1300 / +1201 +1300\n",
      "1301 1400 / +1301 +1400\n",
      "1401 1500 / +1401 +1500\n",
      "1501 1600 / +1501 +1600\n",
      "1601 1700 / +1601 +1700\n",
      "1701 1800 / +1701 +1800\n",
      "1801 1900 / +1801 +1900\n",
      "1901 2000 / +1901 +2000\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "logger.info('Generating date nodes...')\n",
    "with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "    startTimes = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'startTime']\n",
    "    startDict = {}\n",
    "    for line in startTimes:\n",
    "        startDict[line['subject']] = line['object']\n",
    "\n",
    "with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "    endTimes = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'endTime']\n",
    "    endDict = {}\n",
    "    for line in endTimes:\n",
    "        endDict[line['subject']] = line['object']\n",
    "    lkg.close()\n",
    "    \n",
    "    intervalsDict = {}\n",
    "    for k in startDict.keys():\n",
    "        intervalsDict.update({k : (startDict[k], endDict[k])})\n",
    "\n",
    "logger.info('Generatin time points')\n",
    "\n",
    "with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "    timePoints = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'TimePoint']\n",
    "    pointsDict = {}\n",
    "    for line in timePoints:\n",
    "        nodes.addDateNode(line['properties']['Year'], line['identity'], g)\n",
    "        pointsDict[line['identity']] = line['properties']['Year']\n",
    "        \n",
    "with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "    timeIntervals = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'TimeInterval']\n",
    "    for line in timeIntervals:\n",
    "        description = line['properties']['name']\n",
    "        s, e = intervalsDict[line['identity']]\n",
    "        start = pointsDict[s]\n",
    "        end = pointsDict[e]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 18:34:06,305 Generating date nodes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1894\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "logger.info('Generating date nodes...')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
