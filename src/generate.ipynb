{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "import jsonlines\n",
    "import os\n",
    "from rdflib import Graph, Literal, URIRef\n",
    "import oxrdflib\n",
    "import pyoxigraph\n",
    "import nodes\n",
    "import relations\n",
    "import queries\n",
    "from namespaces import *\n",
    "import importlib\n",
    "import utils.etymwn as etymwn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binding namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = Graph(store='Oxigraph')\n",
    "g = Graph()\n",
    "\n",
    "g.bind(\"rdf\", RDF)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dct\", DCTERMS)\n",
    "g.bind(\"owl\", OWL)\n",
    "\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"ontolex\", ONTOLEX)\n",
    "g.bind(\"vartrans\", VARTRANS)\n",
    "g.bind(\"lexinfo\", LEXINFO)\n",
    "g.bind(\"lime\", LIME)\n",
    "g.bind(\"wn\", WORDNET)\n",
    "g.bind(\"lexvo\", LEXVO)\n",
    "g.bind(\"lvont\", LVONT)\n",
    "g.bind(\"uwn\", UWN)\n",
    "g.bind(\"lila\", LILA)\n",
    "g.bind(\"skos\", SKOS)\n",
    "g.bind(\"wd\", WIKIENTITY)\n",
    "g.bind(\"cc\", CC)\n",
    "g.bind(\"llkg\", LLKG)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "llkg = URIRef(LLKG.LLKG)\n",
    "llkgGraph = '../data/llkg/llkg_test_lemma.ttl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupGraph():\n",
    "    g.add((llkg, RDF.type, LIME.Lexicon))\n",
    "    g.add((llkg, RDFS.label, Literal('Linked Linguistic Knowledge Graph', lang='en')))\n",
    "    g.add((llkg, SCHEMA.email, Literal('e.ghizzota@studenti.uniba.it')))\n",
    "    g.add((llkg, CC.license, URIRef('https://creativecommons.org/licenses/by-sa/4.0/deed.en')))\n",
    "\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - EtymWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "etymFolder = '../data/etymwn' \n",
    "lexvoFolder = '../data/lexvo'\n",
    "\n",
    "def languageNodes():\n",
    "    logger.info('Generating language nodes...')\n",
    "\n",
    "    l = Graph()\n",
    "    l.parse(os.path.join(lexvoFolder, 'lexvo_2013-02-09.nt'))\n",
    "    for item in l.subjects(predicate=RDF.type, object=LVONT.Language):\n",
    "        nodes.addLanguageNode(language=item, l=l, g=g)\n",
    "    \n",
    "    g.add((llkg, DCTERMS.language, g.value(subject=None, predicate=RDFS.label, object=Literal(\"English\", lang='en'))))\n",
    "    l.close()\n",
    "\n",
    "    logger.info('Language nodes generated!')\n",
    "    logger.info('Serializing language nodes...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def etymNodes(wordsPath):\n",
    "    logger.info('Generating words nodes...')\n",
    "\n",
    "    file = open(os.path.join(etymFolder, wordsPath), mode='r', encoding='utf-8')\n",
    "    reader = csv.reader(file)\n",
    "    next(reader, None)\n",
    "    for item in reader:\n",
    "        nodes.addLexicalEntryNode(entry=item[1], id=item[0], language=item[2], iso='3',  llkg=llkg, g=g)\n",
    "        if int(item[0]) % 100000 == 0:\n",
    "            logger.info('Still alive! {}'.format(item[0]))\n",
    " \n",
    "    file.close()\n",
    "    logger.info('Word nodes generated!')\n",
    "    logger.info('Serializing EtymWN nodes...')\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etymRelations(wordsDict, dataset):\n",
    "\n",
    "    logger.info('Connecting nodes...')\n",
    "    \n",
    "    for index, value in dataset.iterrows():\n",
    "        subj = str(value['w1']).split(': ')\n",
    "        obj = str(value['w2']).split(': ')\n",
    "        property = str(value['rel']).removeprefix('rel:')\n",
    "        subject = g.value(subject=None, predicate=LLKG.llkgID, object=Literal(wordsDict[subj[1], subj[0]], datatype=XSD.unsignedInt))\n",
    "        object = g.value(subject=None, predicate=LLKG.llkgID, object=Literal(wordsDict[obj[1], obj[0]], datatype=XSD.unsignedInt))\n",
    "\n",
    "        if property == 'etymology':\n",
    "            relations.addEtymology(subject, object, g)\n",
    "            relations.addEtymologicalOrigin(object, subject, g)           \n",
    "        elif property == 'etymologically_related':\n",
    "            relations.addEtymologicallyRelated(subject, object, g)\n",
    "        elif property == 'has_derived_form':\n",
    "            relations.addHasDerivedForm(subject, object, g)\n",
    "            relations.addIsDerivedFrom(subject, object, g)\n",
    "        elif property == 'variant:orthography':\n",
    "            relations.addOrthographyVariant(subject, object, g)\n",
    "\n",
    "        if index % 100000 == 0:\n",
    "            logger.info('Still alive! {}'.format(index))\n",
    "\n",
    "    logger.info('Nodes successfully connected!')\n",
    "    logger.info('Serializing EtymWN relations...')\n",
    "    g.serialize(format='ttl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - LKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkgDataset = '../data/lkg/dataset.jsonl'\n",
    "wikidataMap = '../data/lkg/wikidata_metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resourceNodes():\n",
    "    logger.info('Generating resources nodes...')\n",
    "    nodes.addResourceNode(resource='https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.04.0059', label='Lewis-Short Dictionary', g=g)\n",
    "    nodes.addResourceNode(resource='https://lila-erc.eu/data/lexicalResources/LatinWordNet/Lexicon', label='Latin WordNet', g=g)\n",
    "    nodes.addResourceNode(resource='https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/uwn', label='Universal WordNet', g=g)\n",
    "    logger.info('Serializing resources...')\n",
    "    g.serialize(format='ttl')\n",
    "    \n",
    "def lemmaNodes():\n",
    "    logger.info('Generating lemma nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:   \n",
    "        lemmas = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Lemma')\n",
    "        for line in lemmas:     \n",
    "            nodes.addFormNode(writtenRep=line['properties']['value'], pos=line['properties']['posTag'], id=line['identity'], g=g)      \n",
    "    \n",
    "def entryNodes():\n",
    "    logger.info('Generating entries nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalEntries = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'InflectedWord')  \n",
    "        for line in lexicalEntries:\n",
    "            nodes.addLexicalEntryNode(entry=line['properties']['value'], id=line['identity'], language='lat', iso='3', llkg=llkg, g=g)\n",
    "\n",
    "def senseNodes(): \n",
    "    logger.info('Generating lexical sense nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalSenses = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'LexiconConcept')\n",
    "        for line in lexicalSenses:\n",
    "            resource = line['properties']['resource']\n",
    "            if resource == 'Lewis-Short Dictionary':\n",
    "                nodes.addLexicalSenseNode(resource=resource, sense=line['properties']['id'], gloss=line['properties']['alias'], id=line['identity'], g=g) \n",
    "            elif resource == 'Latin WordNet':\n",
    "                nodes.addLexicalSenseNode(resource='Universal WordNet', sense=line['properties']['alias'], gloss=line['properties']['gloss'], id=line['identity'], g=g) \n",
    "\n",
    "def authorNodes():\n",
    "    logger.info('Generating author nodes...')\n",
    "    authors_df = pd.read_csv(os.path.join(wikidataMap, 'latinISE_author_mapping.tsv'), sep='\\t', header=None, usecols=[2,3,4,5], names=['name', 'lastname', 'title', 'id'])\n",
    "    authors_df = authors_df.drop_duplicates(subset=['id'])\n",
    "    authors_df = authors_df.fillna('')\n",
    "    authors_df['fullname'] = authors_df['name'] + ' ' + authors_df['lastname']\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        authors = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Person']     \n",
    "        for line in authors:\n",
    "           nodes.addPersonNode(firstname=line['properties']['name'], lastname=line['properties']['lastname'], id=line['identity'], df=authors_df, g=g)\n",
    "\n",
    "def occupationNodes():\n",
    "    file = open(os.path.join(wikidataMap, 'occupations_map.tsv'), encoding='utf-8', mode='r')\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    occupationDict = {}\n",
    "    for row in reader:\n",
    "        occupationDict[row[1]] = row[0]\n",
    "    file.close()\n",
    "\n",
    "    logger.info('Generating occupation nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occupations = [line for line in lkg if line['jtype']=='node' and line['label']=='Occupation']        \n",
    "        for line in occupations:\n",
    "           nodes.addOccupationNode(occupation=line['properties']['value'], id=line['identity'], dict=occupationDict, g=g)\n",
    "\n",
    "def textNodes():\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        ids = [line['object'] for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'HAS_OCCURRENCE']\n",
    "    logger.info('Generating text nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occurrences = [line for line in lkg if line['jtype'] == 'node' and line['identity'] in ids]\n",
    "        for line in occurrences:\n",
    "            nodes.addQuotationNode(quotation=line['properties']['value'], language='Latin', id=line['identity'], framework='DuRel' g=g)\n",
    "\n",
    "def documentNodes():\n",
    "    logger.info('Generating document nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        documents = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Document']\n",
    "        for line in documents: # adds dummy nodes\n",
    "            nodes.addDummyBookNode(title=line['properties']['title'], id=line['identity'], g=g)\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        hasAuthor = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'HAS_AUTHOR']\n",
    "        for line in hasAuthor:\n",
    "            document = g.value(predicate=LLKG.llkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            author = g.value(predicate=LLKG.llkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "            nodes.addBookNode(document=document, author=author, g=g)\n",
    "            \n",
    "def corpusNodes():\n",
    "    logger.info('Generating corpora nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        corpora = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Corpus']\n",
    "        for line in corpora:\n",
    "            nodes.addCollectionNode(title=line['properties']['name'], id=line['identity'], g=g)           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateDictionary():\n",
    "    logger.info('Generating dates dictionary...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        startTimes = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'startTime']\n",
    "        startDict = {}\n",
    "        for line in startTimes:\n",
    "            startDict[line['subject']] = line['object']\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        endTimes = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'endTime']\n",
    "        endDict = {}\n",
    "        for line in endTimes:\n",
    "            endDict[line['subject']] = line['object']\n",
    "        \n",
    "        intervalsDict = {}\n",
    "        for k in startDict.keys():\n",
    "            intervalsDict.update({k : (startDict[k], endDict[k])})\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        timePoints = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'TimePoint']\n",
    "        pointsDict = {}\n",
    "        for line in timePoints:\n",
    "            pointsDict[line['identity']] = line['properties']['Year']\n",
    "\n",
    "    return intervalsDict, pointsDict\n",
    "\n",
    "def lkgRelations():\n",
    "\n",
    "    intervalsDict, pointsDict = dateDictionary()\n",
    "\n",
    "    logger.info('Connecting nodes...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        relationships = [line for line in lkg if line['jtype'] == 'relationship']\n",
    "        for line in relationships:\n",
    "            property = line['name']\n",
    "            subj = g.value(predicate=LLKG.llkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            obj = g.value(predicate=LLKG.llkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "\n",
    "            if property == 'HAS_LEMMA':    \n",
    "                relations.addCanonicalForm(subj, obj, g)\n",
    "            elif property == 'HAS_SUBCLASS':\n",
    "                relations.addSenseRel(subj, obj, g)        \n",
    "            elif property == 'SAME_AS':\n",
    "                relations.addSameAs(subj, obj, g)\n",
    "            elif property == 'HAS_AUTHOR':\n",
    "                relations.addAuthor(subj, obj, g)\n",
    "            elif property == 'HAS_OCCUPATION':\n",
    "                relations.addHasOccupation(subj, obj, g)\n",
    "            elif property == 'BELONG_TO' or property == 'HAS_CORPUS':\n",
    "                relations.addSCHEMAIsPartOf(subj, obj, g)\n",
    "            elif property == 'HAS_OCCURRENCE':\n",
    "                occurrence = g.value(subject=None, predicate=DCTERMS.isPartOf, object=obj)\n",
    "                relations.addDCTIsPartOf(subj, occurrence, g)\n",
    "            elif property == 'HAS_EXAMPLE':\n",
    "                example = g.value(subject=None, predicate=DCTERMS.isPartOf, object=obj)\n",
    "                relations.addExample(subj, example, line['properties']['grade'], g)                \n",
    "            elif property == 'PUBLISHED_IN' or property == 'BORN' or property == 'DIED':\n",
    "                if line['object'] in intervalsDict.keys():\n",
    "                    s, e = intervalsDict[line['object']]\n",
    "                    start = pointsDict[s]\n",
    "                    end = pointsDict[e]\n",
    "                    relations.addDateInterval(subj, start, end, property, g)\n",
    "                elif line['object'] in pointsDict.keys():\n",
    "                    relations.addDatePoint(subj, pointsDict[line['object']], property, g)\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        concepts = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'HAS_CONCEPT']\n",
    "\n",
    "        for line in concepts:\n",
    "            subj = g.value(predicate=LLKG.llkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            obj = g.value(predicate=LLKG.llkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "\n",
    "            relations.addSense(subj, obj, g)\n",
    "\n",
    "    logger.info('Nodes successfully connected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(nodes)\n",
    "importlib.reload(relations)\n",
    "importlib.reload(etymwn)\n",
    "importlib.reload(queries)\n",
    "\n",
    "setupGraph()\n",
    "languageNodes()\n",
    "#dataset, entries = etymwn.loadDataset(os.path.join(etymFolder, 'etymwn.tsv'))\n",
    "#etymwn.writeWords(entries, os.path.join(etymFolder, 'words.csv'))\n",
    "#wordsDict = etymwn.loadDictionary(os.path.join(etymFolder, 'words.csv'))\n",
    "#etymNodes(wordsPath='words.csv')\n",
    "resourceNodes()\n",
    "lemmaNodes()\n",
    "entryNodes()\n",
    "senseNodes()\n",
    "#authorNodes()\n",
    "#occupationNodes()\n",
    "#textNodes()\n",
    "#documentNodes()\n",
    "#corpusNodes()\n",
    "logger.info('Serializing nodes...')\n",
    "g.serialize(format='ttl')\n",
    "lkgRelations()\n",
    "logger.info('Serializing graph...')\n",
    "#etymRelations(wordsDict, dataset)\n",
    "g.serialize(destination=llkgGraph,format='ttl')\n",
    "logger.info('{} external links'.format(nodes.externalLinks+relations.externalLinks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
