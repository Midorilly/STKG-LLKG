{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "import jsonlines\n",
    "import os\n",
    "from py4j.java_gateway import JavaGateway\n",
    "from rdflib import Graph, Literal, URIRef, BNode\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD, DCTERMS\n",
    "import re\n",
    "import oxrdflib\n",
    "import pyoxigraph\n",
    "\n",
    "import nodes\n",
    "import relations\n",
    "import queries\n",
    "from namespaces import *\n",
    "import importlib\n",
    "import utils.etymwn as etymwn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binding namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(store='Oxigraph')\n",
    "\n",
    "g.bind(\"rdf\", RDF)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dct\", DCTERMS)\n",
    "g.bind(\"owl\", OWL)\n",
    "\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"ontolex\", ONTOLEX)\n",
    "g.bind(\"vartrans\", VARTRANS)\n",
    "g.bind(\"lexinfo\", LEXINFO)\n",
    "g.bind(\"lime\", LIME)\n",
    "g.bind(\"wn\", WORDNET)\n",
    "g.bind(\"lexvo\", LEXVO)\n",
    "g.bind(\"lvont\", LVONT)\n",
    "g.bind(\"uwn\", UWN)\n",
    "g.bind(\"lila\", LILA)\n",
    "g.bind(\"skos\", SKOS)\n",
    "\n",
    "g.bind(\"wd\", WIKIENTITY)\n",
    "g.bind(\"wdt\", WIKIPROP)\n",
    "g.bind(\"wikibase\", WIKIBASE)\n",
    "g.bind(\"bd\", BIGDATA)\n",
    "\n",
    "g.bind(\"dummy\", DUMMY)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "llkg = URIRef(DUMMY.LLKG)\n",
    "llkgGraph = '../data/llkg/llkg.ttl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupGraph():\n",
    "    g.add((llkg, RDF.type, LIME.Lexicon))\n",
    "    g.add((llkg, RDFS.label, Literal('Linked Linguistic Knowledge Graph', lang='en')))\n",
    "    g.add((llkg, SCHEMA.email, Literal('e.ghizzota@studenti.uniba.it')))\n",
    "\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - EtymWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "etymFolder = '../data/etymwn' \n",
    "\n",
    "def languageNodes():\n",
    "    logger.info('Generating language nodes...')\n",
    "\n",
    "    l = Graph()\n",
    "    l.parse(os.path.join(etymFolder, 'lexvo/lexvo_2013-02-09.nt'))\n",
    "    for item in l.subjects(predicate=RDF.type, object=LVONT.Language):\n",
    "        nodes.addLanguageNode(language=item, l=l, g=g)\n",
    "    \n",
    "    g.add((llkg, DCTERMS.language, g.value(subject=None, predicate=RDFS.label, object=Literal(\"English\", lang='en'))))\n",
    "    l.close()\n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def etymNodes(wordsPath):\n",
    "    logger.info('Generating words nodes...')\n",
    "\n",
    "    file = open(os.path.join(etymFolder, wordsPath), mode='r', encoding='utf-8')\n",
    "    reader = csv.reader(file)\n",
    "    next(reader, None)\n",
    "    for item in reader:\n",
    "        #if item[2] == 'lat':\n",
    "        nodes.addLexicalEntryNode(entry=item[1], id=item[0], language=item[2], iso='3',  llkg=llkg, g=g)\n",
    "        if int(item[0]) % 100000 == 0:\n",
    "            logger.info('Still alive! {}'.format(item[0]))\n",
    "                #g.serialize(format='ttl')\n",
    "    file.close()\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "    ('Word nodes generated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etymRelations(wordsDict, dataset):\n",
    "\n",
    "    logger.info('Connecting nodes...')\n",
    "    \n",
    "    for index, value in dataset.iterrows():\n",
    "        subj = str(value['w1']).split(': ')\n",
    "        obj = str(value['w2']).split(': ')\n",
    "        property = str(value['rel']).removeprefix('rel:')\n",
    "        #if subj[0] == 'lat' and obj[0] == 'lat':\n",
    "        subject = g.value(subject=None, predicate=DUMMY.lkgID, object=Literal(wordsDict[subj[1], subj[0]], datatype=XSD.unsignedInt))\n",
    "        object = g.value(subject=None, predicate=DUMMY.lkgID, object=Literal(wordsDict[obj[1], obj[0]], datatype=XSD.unsignedInt))\n",
    "\n",
    "        if property == 'etymology':\n",
    "            relations.addEtymology(subject, obj, g)\n",
    "            relations.addEtymologicalOrigin(object, subject, g)           \n",
    "        elif property == 'etymologically_related':\n",
    "            relations.addEtymologicallyRelated(subject, object, g)\n",
    "        elif property == 'has_derived_form':\n",
    "            relations.addHasDerivedForm(subject, object, g)\n",
    "            relations.addIsDerivedFrom(subject, object, g)\n",
    "        elif property == 'variant:orthography':\n",
    "            relations.addOrthographyVariant(subject, object, g)\n",
    "\n",
    "        if index % 100000 == 0:\n",
    "            logger.info('Still alive! {}'.format(index))\n",
    "\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "    logger.info('Nodes successfully connected!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etymRelations():\n",
    "    logger.info('Connecting nodes...')\n",
    "\n",
    "    file = open(os.path.join(etymFolder,'relations.csv'), 'r')\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    for line in reader:\n",
    "        subj = g.value(predicate=DUMMY.etymwnID, object=Literal(line[0], datatype=XSD.string))\n",
    "        obj = g.value(predicate=DUMMY.etymwnID, object=Literal(line[2], datatype=XSD.string))\n",
    "\n",
    "        property = line[1]\n",
    "\n",
    "        if property == 'etymology':\n",
    "            relations.addEtymology(subj, obj, g)\n",
    "            relations.addEtymologicalOrigin(obj, subj, g)           \n",
    "        elif property == 'etymologically_related':\n",
    "            relations.addEtymologicallyRelated(subj, obj, g)\n",
    "        elif property == 'has_derived_form':\n",
    "            relations.addHasDerivedForm(subj, obj, g)\n",
    "            relations.addIsDerivedFrom(subj, obj, g)\n",
    "        elif property == 'variant:orthography':\n",
    "            relations.addOrthographyVariant(subj, obj, g)\n",
    "          \n",
    "    file.close()\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')\n",
    "    logger.info('Nodes successfully connected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - LKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkgDataset = '../data/lkg/dataset.jsonl'\n",
    "wikidataMap = '../data/lkg/wikidata_metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resourceNodes():\n",
    "    logger.info('Generating resources nodes...')\n",
    "    nodes.addResourceNode(resource='https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.04.0059', label='Lewis-Short Dictionary', g=g)\n",
    "    nodes.addResourceNode(resource='https://lila-erc.eu/data/lexicalResources/LatinWordNet/Lexicon', label='Latin WordNet', g=g)\n",
    "    nodes.addResourceNode(resource='https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/uwn', label='Universal WordNet', g=g)\n",
    "    '''logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')'''\n",
    "\n",
    "def lemmaNodes():\n",
    "    logger.info('Generating lemma nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:   \n",
    "        lemmas = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Lemma')\n",
    "        for line in lemmas:     \n",
    "            nodes.addFormNode(writtenRep=line['properties']['value'], pos=line['properties']['posTag'], id=line['identity'], g=g)      \n",
    "    '''logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')'''\n",
    "    \n",
    "def entryNodes():\n",
    "    logger.info('Generating entries nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalEntries = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'InflectedWord')  \n",
    "        for line in lexicalEntries:\n",
    "            nodes.addLexicalEntryNode(entry=line['properties']['value'], id=line['identity'], language='lat', iso='3', llkg=llkg, g=g)\n",
    "    '''logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')'''\n",
    "\n",
    "def senseNodes(): \n",
    "    logger.info('Generating lexical sense nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalSenses = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'LexiconConcept')\n",
    "        for line in lexicalSenses:\n",
    "            resource = line['properties']['resource']\n",
    "            if resource == 'Lewis-Short Dictionary':\n",
    "                nodes.addLexicalSenseNode(resource=resource, sense=line['properties']['id'], gloss=line['properties']['alias'], id=line['identity'], g=g) \n",
    "            elif resource == 'Latin WordNet':\n",
    "                nodes.addLexicalSenseNode(resource='Universal WordNet', sense=line['properties']['alias'], gloss=line['properties']['gloss'], id=line['identity'], g=g) \n",
    "    '''logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')'''\n",
    "\n",
    "def authorNodes():\n",
    "    logger.info('Generating author nodes...')\n",
    "    authors_df = pd.read_csv(os.path.join(wikidataMap, 'latinISE_author_mapping.tsv'), sep='\\t', header=None, usecols=[2,3,4,5], names=['name', 'lastname', 'title', 'id'])\n",
    "    authors_df = authors_df.drop_duplicates(subset=['id'])\n",
    "    authors_df = authors_df.fillna('')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        authors = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Person']     \n",
    "        for line in authors:\n",
    "           nodes.addPersonNode(firstname=line['properties']['name'], lastname=line['properties']['lastname'], id=line['identity'], df=authors_df, g=g)\n",
    "    '''logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')'''\n",
    "\n",
    "def occupationNodes():\n",
    "    logger.info('Creating dictionary...')\n",
    "    file = open(os.path.join(wikidataMap, 'occupations_map.tsv'), encoding='utf-8', mode='r')\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    occupationDict = {}\n",
    "    for row in reader:\n",
    "        occupationDict[row[1]] = row[0]\n",
    "    file.close()\n",
    "    logger.info('Dictionary created')\n",
    "\n",
    "    logger.info('Generating occupation nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occupations = [line for line in lkg if line['jtype']=='node' and line['label']=='Occupation']        \n",
    "        for line in occupations:\n",
    "           nodes.addOccupationNode(occupation=line['properties']['value'], id=line['identity'], dict=occupationDict, g=g)\n",
    "    ''' logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')'''\n",
    "\n",
    "def textNodes():\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        ids = [line['object'] for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'HAS_OCCURRENCE']\n",
    "    logger.info('Generating text nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occurrences = [line for line in lkg if line['jtype'] == 'node' and line['identity'] in ids]\n",
    "        textID = 1\n",
    "        for line in occurrences:\n",
    "            nodes.addQuotationNode(quotation=line['properties']['value'], language='Latin', id=line['identity'], g=g)\n",
    "            textID = textID + 1\n",
    "    '''logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')'''\n",
    "\n",
    "def documentNodes():\n",
    "    logger.info('Generating document nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        documents = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Document']\n",
    "        for line in documents:\n",
    "            nodes.addCreativeWorkNode(title=line['properties']['title'], id=line['identity'], g=g)\n",
    "    '''logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')     '''\n",
    "\n",
    "def corpusNodes():\n",
    "    logger.info('Generating corpora nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        corpora = [line for line in lkg if line['jtype'] == 'node' and line['identity'] == 'Corpus']\n",
    "        for line in corpora:\n",
    "            nodes.addCollectionNode(title=line['properties']['name'], id=line['identity'], g=g)      \n",
    "    '''logger.info('Serializing...')\n",
    "    g.serialize(format='ttl') '''           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateDictionary():\n",
    "    logger.info('Generating dates dictionary...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        startTimes = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'startTime']\n",
    "        startDict = {}\n",
    "        for line in startTimes:\n",
    "            startDict[line['subject']] = line['object']\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        endTimes = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'endTime']\n",
    "        endDict = {}\n",
    "        for line in endTimes:\n",
    "            endDict[line['subject']] = line['object']\n",
    "        \n",
    "        intervalsDict = {}\n",
    "        for k in startDict.keys():\n",
    "            intervalsDict.update({k : (startDict[k], endDict[k])})\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        timePoints = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'TimePoint']\n",
    "        pointsDict = {}\n",
    "        for line in timePoints:\n",
    "            pointsDict[line['identity']] = line['properties']['Year']\n",
    "\n",
    "    return intervalsDict, pointsDict\n",
    "\n",
    "def lkgRelations():\n",
    "\n",
    "    intervalsDict, pointsDict = dateDictionary()\n",
    "\n",
    "    logger.info('Connecting nodes...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        relationships = [line for line in lkg if line['jtype'] == 'relationship']\n",
    "\n",
    "        for line in relationships:\n",
    "            property = line['name']\n",
    "            subj = g.value(predicate=DUMMY.lkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            obj = g.value(predicate=DUMMY.lkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "\n",
    "            if property == 'HAS_LEMMA':    \n",
    "                relations.addCanonicalForm(subj, obj, g)\n",
    "            elif property == 'HAS_SUBCLASS':\n",
    "                relations.addSenseRel(subj, obj, g)        \n",
    "            elif property == 'SAME_AS':\n",
    "                relations.addSameAs(subj, obj, g)\n",
    "            elif property == 'HAS_AUTHOR':\n",
    "                relations.addAuthor(subj, obj, g)\n",
    "            elif property == 'HAS_OCCUPATION':\n",
    "                relations.addHasOccupation(subj, obj, g)\n",
    "            elif property == 'BELONG_TO':\n",
    "                relations.addSCHEMAIsPartOf(subj, obj, g)\n",
    "            elif property == 'HAS_OCCURRENCE':\n",
    "                occurrence = g.value(subject=None, predicate=DCTERMS.isPartOf, object=obj)\n",
    "                relations.addDCTIsPartOf(subj, occurrence, g)\n",
    "            elif property == 'HAS_EXAMPLE':\n",
    "                example = g.value(subject=None, predicate=DCTERMS.isPartOf, object=obj)\n",
    "                relations.addExample(subj, example, line['properties']['grade'], g)\n",
    "            elif property == 'PUBLISHED_IN' or property == 'BORN' or property == 'DIED':\n",
    "                if line['object'] in intervalsDict.keys():\n",
    "                    s, e = intervalsDict[line['object']]\n",
    "                    start = pointsDict[s]\n",
    "                    end = pointsDict[e]\n",
    "                    relations.addDateInterval(subj, start, end, property, g)\n",
    "                elif line['object'] in pointsDict.keys():\n",
    "                    relations.addDatePoint(subj, pointsDict[line['object']], property, g)\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        concepts = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'HAS_CONCEPT']\n",
    "\n",
    "        for line in concepts:\n",
    "            subj = g.value(predicate=DUMMY.lkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            obj = g.value(predicate=DUMMY.lkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "\n",
    "            relations.addSense(subj, obj, g)\n",
    "\n",
    "    logger.info('Nodes successfully connected!')\n",
    "    logger.info('Serializing...')\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 11:58:10,417 Generating resources nodes...\n",
      "2024-04-04 11:58:10,437 Generating lemma nodes...\n",
      "2024-04-04 11:58:16,549 Generating entries nodes...\n",
      "2024-04-04 11:58:16,841 Generating lexical sense nodes...\n",
      "2024-04-04 11:58:17,798 Generating dates dictionary...\n",
      "2024-04-04 11:58:18,038 Connecting nodes...\n",
      "2024-04-04 12:02:03,625 Nodes successfully connected!\n",
      "2024-04-04 12:02:03,626 Serializing...\n",
      "2024-04-04 12:02:04,203 Serializing...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N75394fe298c44fe0a85e20f8dc7f48e7 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(nodes)\n",
    "importlib.reload(relations)\n",
    "importlib.reload(etymwn)\n",
    "importlib.reload(queries)\n",
    "\n",
    "g.remove((None, None, None))\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "setupGraph()\n",
    "languageNodes()\n",
    "dataset, entries = etymwn.loadDataset(os.path.join(etymFolder, 'etymwn.tsv'))\n",
    "etymwn.writeWords(entries, os.path.join(etymFolder, 'words.csv'))\n",
    "wordsDict = etymwn.loadDictionary(os.path.join(etymFolder, 'words.csv'))\n",
    "etymNodes(wordsPath='words.csv')\n",
    "etymRelations(wordsDict, dataset)\n",
    "resourceNodes()\n",
    "g.serialize(format='ttl')\n",
    "lemmaNodes()\n",
    "entryNodes()\n",
    "senseNodes()\n",
    "authorNodes()\n",
    "occupationNodes()\n",
    "textNodes()\n",
    "documentNodes()\n",
    "corpusNodes()\n",
    "g.serialize(format='ttl')\n",
    "lkgRelations()\n",
    "\n",
    "logger.info('Serializing...')\n",
    "g.serialize(destination=llkgGraph,format='ttl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
