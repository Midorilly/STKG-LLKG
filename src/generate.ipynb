{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import csv\n",
    "import jsonlines\n",
    "import os\n",
    "from rdflib import Graph, Literal, URIRef\n",
    "import oxrdflib\n",
    "import pyoxigraph\n",
    "import nodes\n",
    "import relations\n",
    "import queries\n",
    "from namespaces import *\n",
    "import importlib\n",
    "import utils.etymwn as etymwn\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### binding namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(store='Oxigraph')\n",
    "#g = Graph()\n",
    "\n",
    "g.bind(\"rdf\", RDF)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dct\", DCTERMS)\n",
    "g.bind(\"owl\", OWL)\n",
    "\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"ontolex\", ONTOLEX)\n",
    "g.bind(\"vartrans\", VARTRANS)\n",
    "g.bind(\"lexinfo\", LEXINFO)\n",
    "g.bind(\"lime\", LIME)\n",
    "g.bind(\"wn\", WORDNET)\n",
    "g.bind(\"lexvo\", LEXVO)\n",
    "g.bind(\"lvont\", LVONT)\n",
    "g.bind(\"uwn\", UWN)\n",
    "g.bind(\"lila\", LILA)\n",
    "g.bind(\"skos\", SKOS08)\n",
    "g.bind(\"wd\", WIKIENTITY)\n",
    "g.bind(\"cc\", CC)\n",
    "g.bind(\"llkg\", LLKG)\n",
    "g.bind(\"wdt\", WIKIPROP)\n",
    "\n",
    "llkg = URIRef(LLKG.LLKG)\n",
    "llkgGraph = '../data/llkg/llkg-missing-sentences.ttl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupGraph():\n",
    "    g.add((llkg, RDF.type, LIME.Lexicon))\n",
    "    g.add((llkg, RDFS.label, Literal('Linked Linguistic Knowledge Graph', lang='en')))\n",
    "    g.add((llkg, SCHEMA.email, Literal('e.ghizzota@studenti.uniba.it')))\n",
    "    g.add((llkg, CC.license, URIRef('https://creativecommons.org/licenses/by-sa/4.0/deed.en')))\n",
    "\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - EtymWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "etymFolder = '../data/etymwn' \n",
    "lexvoFolder = '../data/lexvo'\n",
    "\n",
    "def languageNodes():\n",
    "    l = Graph()\n",
    "    l.bind('skos', SKOS08)\n",
    "    l.parse(os.path.join(lexvoFolder, 'lexvo_2013-02-09.nt'))\n",
    "    logger.info('Generating language nodes...')\n",
    "    for item in l.subjects(predicate=RDF.type, object=LVONT.Language):\n",
    "        nodes.addLanguageNode(language=item, l=l, g=g)\n",
    "    english = g.value(subject=None, predicate=RDFS.label, object=Literal('English', lang='en'))\n",
    "    g.add((llkg, DCTERMS.language, english))\n",
    "    l.close()\n",
    "    logger.info('Language nodes generated!')\n",
    "    logger.info('Serializing language nodes...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def etymNodes(wordsPath):\n",
    "    logger.info('Generating words nodes...')\n",
    "\n",
    "    file = open(os.path.join(etymFolder, wordsPath), mode='r', encoding='utf-8')\n",
    "    reader = csv.reader(file)\n",
    "    next(reader, None)\n",
    "    for item in reader:\n",
    "        nodes.addLexicalEntryNode(entry=item[1], id=item[0], language=item[2], iso='3',  llkg=llkg, g=g)\n",
    "        if int(item[0]) % 100000 == 0:\n",
    "            logger.info('Still alive! {}'.format(item[0]))\n",
    " \n",
    "    file.close()\n",
    "    logger.info('Word nodes generated!')\n",
    "    logger.info('Serializing EtymWN nodes...')\n",
    "    g.serialize(format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etymRelations(wordsDict, dataset):\n",
    "\n",
    "    logger.info('Connecting nodes...')\n",
    "    \n",
    "    for index, value in dataset.iterrows():\n",
    "        subj = str(value['w1']).split(': ')\n",
    "        obj = str(value['w2']).split(': ')\n",
    "        property = str(value['rel']).removeprefix('rel:')\n",
    "        subject = g.value(subject=None, predicate=LLKG.llkgID, object=Literal(wordsDict[subj[1], subj[0]], datatype=XSD.unsignedInt))\n",
    "        object = g.value(subject=None, predicate=LLKG.llkgID, object=Literal(wordsDict[obj[1], obj[0]], datatype=XSD.unsignedInt))\n",
    "\n",
    "        if property == 'etymology':\n",
    "            relations.addEtymology(subject, object, g)\n",
    "            relations.addEtymologicalOrigin(object, subject, g)           \n",
    "        elif property == 'etymologically_related':\n",
    "            relations.addEtymologicallyRelated(subject, object, g)\n",
    "        elif property == 'has_derived_form':\n",
    "            relations.addHasDerivedForm(subject, object, g)\n",
    "            relations.addIsDerivedFrom(subject, object, g)\n",
    "        elif property == 'variant:orthography':\n",
    "            relations.addOrthographyVariant(subject, object, g)\n",
    "\n",
    "        if index % 100000 == 0:\n",
    "            logger.info('Still alive! {}'.format(index))\n",
    "\n",
    "    logger.info('Nodes successfully connected!')\n",
    "    logger.info('Serializing EtymWN relations...')\n",
    "    g.serialize(format='ttl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset - LKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lkgDataset = '../data/lkg/original-dataset.jsonl'\n",
    "wikidataMap = '../data/lkg/wikidata_metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resourceNodes():\n",
    "    logger.info('Generating resources nodes...')\n",
    "    nodes.addResourceNode(resource='https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.04.0059', label='Lewis-Short Dictionary', g=g)\n",
    "    nodes.addResourceNode(resource='https://lila-erc.eu/data/lexicalResources/LatinWordNet/Lexicon', label='Latin WordNet', g=g)\n",
    "    nodes.addResourceNode(resource='https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/uwn', label='Universal WordNet', g=g)\n",
    "    logger.info('Serializing resources...')\n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "def frameworkNodes():\n",
    "    nodes.addFrameworkNode(framework='DuRel', g=g)\n",
    "    g.serialize(format='ttl')\n",
    "    \n",
    "def lemmaNodes():\n",
    "    logger.info('Generating lemma nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:   \n",
    "        lemmas = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Lemma')\n",
    "        for line in lemmas:     \n",
    "            nodes.addFormNode(writtenRep=line['properties']['value'], pos=line['properties']['posTag'], id=line['identity'], llkg=llkg, g=g)      \n",
    "    \n",
    "def entryNodes():\n",
    "    logger.info('Generating entries nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalEntries = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'InflectedWord')  \n",
    "        for line in lexicalEntries:\n",
    "            nodes.addLexicalEntryNode(entry=line['properties']['value'], id=line['identity'], language='lat', iso='3', llkg=llkg, g=g)\n",
    "\n",
    "def senseNodes(): \n",
    "    logger.info('Generating lexical sense nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        lexicalSenses = (line for line in lkg if line['jtype'] == 'node' and line['label'] == 'LexiconConcept')\n",
    "        for line in lexicalSenses:\n",
    "            resource = line['properties']['resource']\n",
    "            if resource == 'Lewis-Short Dictionary':\n",
    "                nodes.addLexicalSenseNode(resource=resource, sense=line['properties']['id'], gloss=line['properties']['alias'], id=line['identity'], g=g) \n",
    "            elif resource == 'Latin WordNet':\n",
    "                nodes.addLexicalSenseNode(resource='Universal WordNet', sense=line['properties']['alias'], gloss=line['properties']['gloss'], id=line['identity'], g=g) \n",
    "\n",
    "def authorNodes():\n",
    "    logger.info('Generating author nodes...')\n",
    "    authors_df = pd.read_csv(os.path.join(wikidataMap, 'latinISE_author_mapping.tsv'), sep='\\t', header=None, usecols=[2,3,4,5], names=['name', 'lastname', 'title', 'id'])\n",
    "    authors_df = authors_df.drop_duplicates(subset=['id'])\n",
    "    authors_df = authors_df.fillna('')\n",
    "    authors_df['fullname'] = authors_df['name'] + ' ' + authors_df['lastname']\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        authors = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Person']     \n",
    "        for line in authors:\n",
    "           nodes.addPersonNode(firstname=line['properties']['name'], lastname=line['properties']['lastname'], id=line['identity'], df=authors_df, g=g)\n",
    "\n",
    "def occupationNodes():\n",
    "    file = open(os.path.join(wikidataMap, 'occupations_map.tsv'), encoding='utf-8', mode='r')\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    occupationDict = {}\n",
    "    for row in reader:\n",
    "        occupationDict[row[1]] = row[0]\n",
    "    file.close()\n",
    "\n",
    "    logger.info('Generating occupation nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occupations = [line for line in lkg if line['jtype']=='node' and line['label']=='Occupation']        \n",
    "        for line in occupations:\n",
    "           nodes.addOccupationNode(occupation=line['properties']['value'], id=line['identity'], dict=occupationDict, g=g)\n",
    "\n",
    "def textNodes():\n",
    "    logger.info('Generating text nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        occurrences = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Text']\n",
    "        for line in occurrences:\n",
    "            nodes.addQuotationNode(quotation=line['properties']['value'], language='Latin', id=line['identity'], g=g)\n",
    "\n",
    "def documentNodes():\n",
    "    logger.info('Generating document nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        documents = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Document']\n",
    "        for line in documents: # adds dummy nodes\n",
    "            nodes.addDummyBookNode(title=line['properties']['title'], id=line['identity'], g=g)\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        hasAuthor = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'HAS_AUTHOR']\n",
    "        for line in hasAuthor:\n",
    "            document = g.value(predicate=LLKG.llkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            author = g.value(predicate=LLKG.llkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "            nodes.addBookNode(document=document, author=author, g=g)\n",
    "            \n",
    "def corpusNodes():\n",
    "    logger.info('Generating corpora nodes...')\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        corpora = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'Corpus']\n",
    "        for line in corpora:\n",
    "            nodes.addCollectionNode(title=line['properties']['name'], id=line['identity'], g=g)           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateDictionary():\n",
    "    logger.info('Generating dates dictionary...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        startTimes = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'startTime']\n",
    "        startDict = {}\n",
    "        for line in startTimes:\n",
    "            startDict[line['subject']] = line['object']\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        endTimes = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'endTime']\n",
    "        endDict = {}\n",
    "        for line in endTimes:\n",
    "            endDict[line['subject']] = line['object']\n",
    "        \n",
    "        intervalsDict = {}\n",
    "        for k in startDict.keys():\n",
    "            intervalsDict.update({k : (startDict[k], endDict[k])})\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        timePoints = [line for line in lkg if line['jtype'] == 'node' and line['label'] == 'TimePoint']\n",
    "        pointsDict = {}\n",
    "        for line in timePoints:\n",
    "            pointsDict[line['identity']] = line['properties']['Year']\n",
    "\n",
    "    return intervalsDict, pointsDict\n",
    "\n",
    "def lkgRelations():\n",
    "\n",
    "    intervalsDict, pointsDict = dateDictionary()\n",
    "\n",
    "    logger.info('Connecting nodes...')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        relationships = [line for line in lkg if line['jtype'] == 'relationship']\n",
    "        for line in relationships:\n",
    "            property = line['name']\n",
    "            subj = g.value(predicate=LLKG.llkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            obj = g.value(predicate=LLKG.llkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "\n",
    "            if property == 'HAS_LEMMA':    \n",
    "                relations.addCanonicalForm(subj, obj, line['subject'], line['object'], g)\n",
    "    \n",
    "    g.serialize(format='ttl')\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        relationships = [line for line in lkg if line['jtype'] == 'relationship']\n",
    "        for line in relationships:\n",
    "            property = line['name']\n",
    "            subj = g.value(predicate=LLKG.llkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            obj = g.value(predicate=LLKG.llkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "\n",
    "            if property == 'HAS_SUBCLASS':\n",
    "                relations.addSenseRel(subj, obj, g)        \n",
    "            elif property == 'SAME_AS':\n",
    "                relations.addSameAs(subj, obj, g)\n",
    "            elif property == 'HAS_AUTHOR':\n",
    "                relations.addAuthor(subj, obj, g)\n",
    "            elif property == 'HAS_OCCUPATION':\n",
    "                relations.addHasOccupation(subj, obj, g)\n",
    "            elif property == 'BELONG_TO' or property == 'HAS_CORPUS':\n",
    "                relations.addSCHEMAIsPartOf(subj, obj, g)\n",
    "            elif property == 'HAS_OCCURRENCE':\n",
    "                relations.addDCTIsPartOf(subj, obj, line['subject'], line['object'], g)\n",
    "            elif property == 'HAS_EXAMPLE': \n",
    "                relations.addExample(subj, obj, line['properties']['grade'], line['object'], g)     \n",
    "            elif property == 'PUBLISHED_IN' or property == 'BORN' or property == 'DIED':\n",
    "                if line['object'] in intervalsDict.keys():\n",
    "                    s, e = intervalsDict[line['object']]\n",
    "                    start = pointsDict[s]\n",
    "                    end = pointsDict[e]\n",
    "                    relations.addDateInterval(subj, start, end, property, g)\n",
    "                elif line['object'] in pointsDict.keys():\n",
    "                    relations.addDatePoint(subj, pointsDict[line['object']], property, g)\n",
    "\n",
    "    with jsonlines.open(lkgDataset, 'r') as lkg:\n",
    "        concepts = [line for line in lkg if line['jtype'] == 'relationship' and line['name'] == 'HAS_CONCEPT']\n",
    "\n",
    "        for line in concepts:\n",
    "            subj = g.value(predicate=LLKG.llkgID, object=Literal(line['subject'], datatype=XSD.unsignedInt))\n",
    "            obj = g.value(predicate=LLKG.llkgID, object=Literal(line['object'], datatype=XSD.unsignedInt))\n",
    "\n",
    "            relations.addSense(subj, obj, g)\n",
    "\n",
    "    logger.info('Nodes successfully connected!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(nodes)\n",
    "importlib.reload(relations)\n",
    "importlib.reload(etymwn)\n",
    "importlib.reload(queries)\n",
    "\n",
    "setupGraph()\n",
    "languageNodes()\n",
    "dataset, entries = etymwn.loadDataset(os.path.join(etymFolder, 'etymwn.tsv'))\n",
    "etymwn.writeWords(entries, os.path.join(etymFolder, 'words.csv'))\n",
    "wordsDict = etymwn.loadDictionary(os.path.join(etymFolder, 'words.csv'))\n",
    "etymNodes(wordsPath='words.csv')\n",
    "etymRelations(wordsDict, dataset)\n",
    "resourceNodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 12:22:53,073 Generating language nodes...\n",
      "2024-07-04 12:22:56,876 Language nodes generated!\n",
      "2024-07-04 12:22:56,877 Serializing language nodes...\n",
      "2024-07-04 12:22:59,498 Generating resources nodes...\n",
      "2024-07-04 12:22:59,500 Serializing resources...\n",
      "2024-07-04 12:23:04,914 Generating lemma nodes...\n",
      "2024-07-04 12:25:24,059 Generating entries nodes...\n",
      "2024-07-04 12:25:24,328 Generating lexical sense nodes...\n",
      "2024-07-04 12:25:26,732 Generating author nodes...\n",
      "2024-07-04 12:25:51,976 len 1\n",
      "2024-07-04 12:25:51,978 Retrieving time periods for Q234593\n",
      "2024-07-04 12:25:55,652 len 0\n",
      "2024-07-04 12:25:55,653 Retrieving time periods for Q43689\n",
      "2024-07-04 12:25:58,909 len 1\n",
      "2024-07-04 12:25:58,910 Retrieving time periods for Q8018\n",
      "2024-07-04 12:25:59,499 len 1\n",
      "2024-07-04 12:25:59,500 Retrieving time periods for Q1414\n",
      "2024-07-04 12:25:59,543 Generating occupation nodes...\n",
      "2024-07-04 12:25:59,654 Generating text nodes...\n",
      "2024-07-04 12:26:02,495 Generating document nodes...\n",
      "2024-07-04 12:27:22,744 Generating corpora nodes...\n",
      "2024-07-04 12:27:22,788 Serializing nodes...\n",
      "2024-07-04 12:27:30,602 Generating dates dictionary...\n",
      "2024-07-04 12:27:30,746 Connecting nodes...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "addCanonicalForm() missing 1 required positional argument: 'g'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSerializing nodes...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m g\u001b[38;5;241m.\u001b[39mserialize(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mttl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mlkgRelations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSerializing graph...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m g\u001b[38;5;241m.\u001b[39mserialize(destination\u001b[38;5;241m=\u001b[39mllkgGraph,\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mttl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 43\u001b[0m, in \u001b[0;36mlkgRelations\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m obj \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mvalue(predicate\u001b[38;5;241m=\u001b[39mLLKG\u001b[38;5;241m.\u001b[39mllkgID, \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m=\u001b[39mLiteral(line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m], datatype\u001b[38;5;241m=\u001b[39mXSD\u001b[38;5;241m.\u001b[39munsignedInt))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mproperty\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHAS_LEMMA\u001b[39m\u001b[38;5;124m'\u001b[39m:    \n\u001b[1;32m---> 43\u001b[0m     \u001b[43mrelations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddCanonicalForm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mproperty\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHAS_SUBCLASS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     45\u001b[0m     relations\u001b[38;5;241m.\u001b[39maddSenseRel(subj, obj, g)        \n",
      "\u001b[1;31mTypeError\u001b[0m: addCanonicalForm() missing 1 required positional argument: 'g'"
     ]
    }
   ],
   "source": [
    "importlib.reload(nodes)\n",
    "importlib.reload(relations)\n",
    "importlib.reload(etymwn)\n",
    "importlib.reload(queries)\n",
    "\n",
    "g.remove((None, None, None))\n",
    "\n",
    "setupGraph()\n",
    "languageNodes()\n",
    "resourceNodes()\n",
    "frameworkNodes()\n",
    "lemmaNodes()\n",
    "entryNodes()\n",
    "senseNodes()\n",
    "authorNodes()\n",
    "occupationNodes()\n",
    "textNodes()\n",
    "documentNodes()\n",
    "corpusNodes()\n",
    "logger.info('Serializing nodes...')\n",
    "g.serialize(format='ttl')\n",
    "lkgRelations()\n",
    "logger.info('Serializing graph...')\n",
    "g.serialize(destination=llkgGraph,format='ttl')\n",
    "logger.info('{}, {} external links'.format(nodes.links, relations.relationslinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 11:15:06,476 Loading words...\n",
      "2024-05-07 11:15:10,508 Words loaded\n"
     ]
    }
   ],
   "source": [
    "dataset, entries = etymwn.loadDataset(os.path.join(etymFolder, 'etymwn.tsv'))\n",
    "etymwn.writeWords(entries, os.path.join(etymFolder, 'words.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = Graph(store='Oxigraph')\n",
    "gr.parse('../data/llkg/llkg-test.ttl', format='ttl')\n",
    "\n",
    "\n",
    "gr.bind(\"rdf\", RDF)\n",
    "gr.bind(\"rdfs\", RDFS)\n",
    "gr.bind(\"xsd\", XSD)\n",
    "gr.bind(\"dct\", DCTERMS)\n",
    "gr.bind(\"owl\", OWL)\n",
    "\n",
    "gr.bind(\"schema\", SCHEMA)\n",
    "gr.bind(\"ontolex\", ONTOLEX)\n",
    "gr.bind(\"vartrans\", VARTRANS)\n",
    "gr.bind(\"lexinfo\", LEXINFO)\n",
    "gr.bind(\"lime\", LIME)\n",
    "gr.bind(\"wn\", WORDNET)\n",
    "gr.bind(\"lexvo\", LEXVO)\n",
    "gr.bind(\"lvont\", LVONT)\n",
    "gr.bind(\"uwn\", UWN)\n",
    "gr.bind(\"lila\", LILA)\n",
    "gr.bind(\"skos\", SKOS08)\n",
    "gr.bind(\"wd\", WIKIENTITY)\n",
    "gr.bind(\"cc\", CC)\n",
    "gr.bind(\"llkg\", LLKG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 11:17:02,383 - arma contingere. suos quaeque\n",
      "2024-06-03 11:17:02,384 -5932773103667000748\n"
     ]
    }
   ],
   "source": [
    "idQuery = ''' SELECT ?id\n",
    "    WHERE {{\n",
    "        ?text rdf:type schema:Quotation ;\n",
    "            schema:text ?sentence ;\n",
    "            llkg:llkgID ?id .\n",
    "        FILTER(regex(?sentence, \"{}\")).\n",
    "    }}\n",
    "'''\n",
    "\n",
    "def executeQuery(q: str, text: str):\n",
    "    output = g.query(q.format(text), initNs = {'schema' : SCHEMA, 'llkg' : LLKG, 'rdf' : RDF})\n",
    "    return output\n",
    "sentence = \"- arma contingere. suos quaeque regio propria distinguit nota, quam abiecisse capitale est, ut vel extra suos conspici fines, vel cum alterius regionis servo quicquam esse collocutum. at neque tutior fugae meditatio quam ipsa est fuga. quin conscium talis fuisse consilii in servo nex est; in libero servitus. contra indici praemia decreta sunt; libero pecunia, servo libertas. utrique vero venia atque impunitas conscientiae, ne quando persequi malum consilium quam poenitere sit tutius. Huius rei haec lex atque hic ordo est, quem dixi. qui quantum habeat TARGET humanitatis\t et commodi, facile patet. quando sic irascitur, ut vitia perimat servatis hominibus, atque ita tractatis, ut bonos esse necesse sit. et quantum ante damni dederunt, tantum reliqua vita resartiant. Porro ne ad pristinos relabantur mores, adeo nullus est metus, ut viatores quoque quibus iter aliquo institutum est, non aliis viae ducibus sese tutioreis arbitrentur, quam servis illis ad quamque regionem subinde commutatis. nempe ad perpetrandum latrocinium nihil habent usquam /non importunum; manus inermes; pecunia tantum sceleris index; deprehenso parata vindicta; neque spes ulla\".split(' ', 5)\n",
    "x = ' '.join(sentence[:5])\n",
    "logger.info(x)\n",
    "logger.info(str(hash(x)))\n",
    "#output = executeQuery(idQuery, sentence[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
